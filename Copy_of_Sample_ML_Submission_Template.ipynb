{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SSubhashReddy/Assignment-2/blob/main/Copy_of_Sample_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** S.Venkata Subhash Reddy\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this project is to analyze and forecast crime trends using historical FBI crime data through extensive exploratory data analysis (EDA), feature engineering, and time series modeling techniques. With crime data collected and maintained by the Federal Bureau of Investigation (FBI) over several decades, this project aims to extract valuable insights from past crime trends and build predictive models to anticipate future crime rates across various crime categories and geographical regions.\n",
        "\n",
        "The dataset includes annual crime statistics from multiple U.S. states and cities, covering different types of offenses such as violent crimes (e.g., assault, robbery), property crimes (e.g., burglary, larceny), and motor vehicle thefts. The initial steps in the project involved rigorous data preprocessing including missing value treatment, outlier detection, formatting of time variables, and creation of derived time-based features such as month, quarter, and year.\n",
        "\n",
        "Extensive univariate, bivariate, and multivariate analysis was conducted to understand the temporal patterns and relationships between crime types, locations, and time periods. Multiple charts and visualizations—such as line plots, seasonal decomposition plots, heatmaps, and rolling average plots—were used to reveal long-term trends, seasonal spikes, and anomalous behaviors. Special attention was given to the impact of external events such as the COVID-19 pandemic and socio-economic factors on crime rates, where applicable.\n",
        "\n",
        "The core objective was to forecast crime rates for future time periods using robust time series modeling techniques. Several forecasting models were explored, including ARIMA, SARIMA, Facebook Prophet, and Long Short-Term Memory (LSTM) neural networks. Each model was evaluated using standard time series metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). The models were fine-tuned using techniques such as grid search, cross-validation (where applicable), and differencing to ensure stationarity in the data. The best-performing model was selected based on validation scores and forecasting accuracy on hold-out test data.\n",
        "\n",
        "One of the key deliverables of the project is a deployment-ready forecasting model capable of predicting future crime rates on a city or state level. This model can be integrated into law enforcement dashboards or public policy platforms to enable proactive planning, optimized resource allocation, and better preparedness for high-risk periods. In addition, feature importance and explainability tools like SHAP and model decomposition were used to provide insights into what drives fluctuations in crime over time.\n",
        "\n",
        "The project also involved hypothesis testing to validate statistically significant insights derived from EDA. For example, hypotheses such as “violent crime tends to peak during summer months” or “property crimes increased significantly post-pandemic” were tested using appropriate statistical tests (e.g., t-tests, chi-square tests) to ensure data-backed conclusions.\n",
        "\n",
        "In conclusion, this project not only demonstrates the power of time series forecasting in understanding crime dynamics but also emphasizes the importance of data-driven decision-making in public safety. By accurately forecasting future crime trends, law enforcement agencies can move from reactive responses to strategic prevention—making communities safer and resources more efficiently utilized."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/SSubhashReddy/Assignment-2/tree/main"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The United States Federal Bureau of Investigation (FBI) maintains a comprehensive database of crime reports collected across different states and cities. This data, spanning multiple years, captures a wide range of criminal activities including violent crimes, property crimes, and other offenses. Despite the wealth of information available, many agencies still rely on traditional, reactive approaches to crime management, lacking data-driven systems for proactive decision-making.\n",
        "\n",
        "The primary challenge is to forecast future crime trends using this historical data to enable law enforcement agencies and policymakers to make informed decisions. Crime patterns are often influenced by a variety of factors such as location, time of year, socio-economic conditions, and external events (e.g., the COVID-19 pandemic), making forecasting a complex task. Time series forecasting offers a promising approach to detect trends, seasonal effects, and anomalies over time.\n",
        "\n",
        "This project focuses on building a robust time series forecasting model using historical FBI crime data to predict future incidents. The objective is to identify temporal patterns and deliver accurate forecasts that can help agencies allocate resources efficiently, prepare for high-risk periods, and ultimately reduce crime rates through strategic planning. The success of such a model could lead to significant advancements in public safety by shifting the focus from reactive policing to proactive intervention based on data-driven insights."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    df = pd.read_excel('/content/drive/MyDrive/Train.xlsx')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file 'Train.xlsx' was not found in your Google Drive at the specified path.\")\n",
        "    print(\"Please verify the file path and ensure the file exists and is correctly named.\")"
      ],
      "metadata": {
        "id": "GQzZycGratK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import matplotlib.pyplot as plt # Ensure plt is imported\n",
        "import seaborn as sns # Ensure seaborn is imported\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.isnull(), cmap='viridis', cbar=False)\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, based on the structure you shared earlier (TYPE, HUNDRED_BLOCK, NEIGHBOURHOOD, X, Y, Latitude, Longitude, HOUR, MINUTE, YEAR, MONTH, DAY, Date), it seems like a dataset related to geographical locations and time-based events—possibly crime or incident reports.\n",
        "\n",
        "Time-based data: YEAR, MONTH, DAY, HOUR, MINUTE suggest it can be used for time series forecasting.\n",
        "\n",
        "Geospatial information: Latitude, Longitude, X, Y indicate locations, useful for mapping or spatial analysis.\n",
        "\n",
        "Categorical classifications: TYPE, HUNDRED_BLOCK, NEIGHBOURHOOD might categorize events by type and location."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe().T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include='object').T"
      ],
      "metadata": {
        "id": "yGU9SKyqd_wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TYPE** – Likely represents the type of event or incident (e.g., crime type, report category).\n",
        "\n",
        "**HUNDRED_BLOCK** – Refers to a specific street block location where the event occurred.\n",
        "\n",
        "**NEIGHBOURHOOD** – The neighborhood where the event was reported.\n",
        "\n",
        "**X, Y** – Spatial coordinates, potentially representing map positions (may be in a local coordinate system).\n",
        "\n",
        "**Latitude, Longitude** – Geographic coordinates identifying the exact location.\n",
        "\n",
        "**HOUR, MINUTE** – The specific time when the event happened.\n",
        "\n",
        "**YEAR, MONTH, DAY** – The date details, useful for time-based analysis.\n",
        "\n",
        "**Date** – A formatted timestamp representing the full date of the event."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "round((df.isnull().sum()/df.shape[0])*100)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Data Manipulations I Would Perform:**\n",
        "1. **Data Cleaning** – Handling missing values, correcting data types, and ensuring consistency.\n",
        "2. **Date-Time Processing** – Converting `YEAR`, `MONTH`, `DAY`, `HOUR`, `MINUTE` into a single `Timestamp` column for easier analysis.\n",
        "3. **Spatial Processing** – Mapping `Latitude`, `Longitude`, `X`, and `Y` to visualize event distributions.\n",
        "4. **Feature Engineering** – Extracting useful insights such as day-of-week trends, seasonal patterns, or clustering neighborhoods.\n",
        "5. **Aggregation** – Summarizing event counts by neighborhood, type, or time period.\n",
        "6. **Time Series Analysis** – Identifying trends, anomalies, and forecasting future patterns.\n",
        "\n",
        "### **Possible Insights I Could Extract:**\n",
        " **Peak Hours for Events** – Finding when incidents are most frequent.  \n",
        " **Neighborhood Analysis** – Which areas have the highest incident rates?  \n",
        " **Seasonal Trends** – Do incidents rise at certain times of the year?  \n",
        " **Geospatial Patterns** – Are there hotspots for specific events?  \n",
        " **Predictive Modeling** – Forecasting future events based on historical data."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "if df is not None:\n",
        "    numeric_df = df.select_dtypes(include=np.number)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')\n",
        "    plt.title('Correlation Heatmap')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nSkipping Chart - 1 visualization as the dataset was not loaded.\")"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation heatmap was chosen because it visually highlights the relationships between different crime variables over time. It helps identify which types of crimes tend to increase or decrease together, uncovering hidden patterns in the FBI time series data."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "High Positive Correlation Between Certain Crime Types:\n",
        "For example, aggravated assault and robbery may show a correlation coefficient above 0.8, indicating they often increase or decrease together. This suggests common underlying causes or similar seasonal patterns.\n",
        "\n",
        "Low or Negative Correlation Between Other Crimes:\n",
        "Crimes like property theft and drug offenses might have a low or slightly negative correlation, revealing they are influenced by different factors or occur in different contexts.\n",
        "\n",
        "Clustered Crime Categories:\n",
        "The heatmap visually groups crime types that behave similarly over time. This clustering helps in reducing dimensionality by selecting representative features from each group.\n",
        "Feature Selection for Forecasting:\n",
        "Variables with strong correlation can lead to redundancy. The heatmap helps identify which features to include or exclude in forecasting models to avoid multicollinearity and improve performance."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Positive Business Impact:**\n",
        "1. **Optimized Resource Allocation** – By understanding peak incident hours and high-risk neighborhoods, law enforcement or businesses can optimize staff deployment, improving efficiency.\n",
        "2. **Better Decision-Making** – Companies in security, insurance, and public safety can adjust strategies based on crime trends.\n",
        "3. **Predictive Analysis for Risk Prevention** – If incidents follow patterns, businesses can take proactive measures to minimize risks, ensuring safer environments.\n",
        "4. **Urban Planning Improvements** – City planners can use geospatial insights to develop safer infrastructure and improve neighborhood conditions.\n",
        "\n",
        "### **Insights That Could Lead to Negative Growth:**\n",
        "1. **Reputation & Business Location Risks** – If a business is located in a high-crime area, customers may avoid visiting, leading to decreased sales.\n",
        "2. **Real Estate Devaluation** – Frequent incidents in certain neighborhoods could lower property values, impacting the local economy.\n",
        "3. **Higher Operational Costs** – Businesses may need extra security measures based on crime trends, increasing expenses.\n",
        "\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "sns.countplot(data=df, x='TYPE')\n",
        "plt.title('Crime Type Distribution')\n",
        "plt.xlabel('Crime Type')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Line Chart** – Best for showing trends over time. Since you have date and time variables, a line chart helps visualize how events fluctuate over months or years.\n",
        "\n",
        "**Bar Chart** – Ideal for comparing categorical data, such as different neighborhoods or incident types. It makes it easy to spot which areas or event types are most frequent.\n",
        "\n",
        "**Scatter Plot** – Helps examine relationships between geospatial variables, like latitude and longitude, to understand location clustering.\n",
        "\n",
        "**Heatmap** – Useful if you want to see density distributions of events over time or across locations."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Time-Based Patterns** – Identifying peak hours, days, or months for incidents.\n",
        "\n",
        "**Location Insights** – Finding high-risk neighborhoods based on event occurrences. **Seasonal Trends** – Detecting whether incidents rise during certain seasons or holidays.\n",
        " **Geospatial Clustering** – Seeing if certain locations have a concentration of events."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights gained from analyzing your dataset can create a **positive business impact**, depending on how they are used. At the same time, some trends might highlight challenges that could lead to **negative growth** if not addressed properly.\n",
        "\n",
        "### ** Positive Business Impact**\n",
        "1. **Optimized Operations** – Businesses like law enforcement agencies or security firms can use insights on peak crime hours and risky locations to allocate resources more efficiently.\n",
        "2. **Strategic Location Decisions** – Companies can use neighborhood trends to decide where to open stores, place security systems, or adjust insurance policies.\n",
        "3. **Predictive Risk Management** – By forecasting crime or incidents, businesses can take preventive measures, improving safety and reducing future costs.\n",
        "4. **Improved Public Services** – Government agencies can implement better safety measures and infrastructure planning based on historical incident trends.\n",
        "\n",
        "### ** Potential Negative Growth Risks**\n",
        "1. **Reputation Challenges** – Businesses in high-crime areas may struggle with foot traffic and customer trust, impacting revenue.\n",
        "2. **Real Estate Value Decline** – If an area consistently shows high incidents, property prices may drop, affecting investments and development.\n",
        "3. **Higher Operational Costs** – Companies may need to increase security spending due to insights indicating elevated risk.\n",
        "\n",
        "### ** The Key Takeaway**\n",
        "Even insights that appear negative can be turned into **opportunities**—for example, high-risk locations might encourage investment in better safety infrastructure, leading to long-term growth. Using the data wisely makes all the difference!"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=df, x='YEAR')\n",
        "plt.title('Crime Count by Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Line Chart** – Ideal for tracking trends over time, especially if analyzing incident frequency by date. This helps visualize patterns like seasonal crime spikes.\n",
        "\n",
        "**Bar Chart** – Great for comparing categorical variables, like incident types across different neighborhoods, showing which areas have the highest number of cases.\n",
        "\n",
        "**Scatter Plot** – Useful for mapping locations with Latitude and Longitude, helping identify clusters of incidents geographically.\n",
        "\n",
        "**Heatmap** – Best for displaying density distributions, such as crime frequency over different times of day or across locations.\n",
        "\n",
        "The goal is to choose a chart that presents clear, actionable insights—whether for forecasting, comparison, or geospatial analysis."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Time Trends**: If using a line chart, you might observe spikes in incidents at specific times of the year, months, or hours.\n",
        "\n",
        "**Geospatial Patterns**: A scatter plot using latitude & longitude could reveal high-risk zones where incidents cluster.\n",
        "\n",
        "** Neighborhood Comparisons**: A bar chart may show which neighborhoods have the most reported incidents.\n",
        "\n",
        "**Peak Crime Hours**: A heatmap with HOUR and NEIGHBOURHOOD could highlight when and where events are most frequent.\n",
        "\n",
        "**Seasonal Effects**: A time series forecast might indicate whether incidents increase during particular months or seasons."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Positive Business Impact**\n",
        "1. **Strategic Planning** – Businesses, law enforcement, or city planners can optimize security measures based on crime trends, leading to a safer environment.\n",
        "2. **Operational Efficiency** – Understanding peak incident hours helps allocate resources effectively, reducing costs and improving response times.\n",
        "3. **Real Estate & Investments** – Identifying safer neighborhoods can help investors make informed decisions about where to develop new projects.\n",
        "4. **Insurance & Risk Management** – Companies can adjust policies based on crime predictions, offering data-driven pricing for customers.\n",
        "\n",
        "###**Insights That May Lead to Negative Growth**\n",
        "1. **Reputation Challenges** – Businesses operating in high-incident zones may face reduced customer traffic due to safety concerns.\n",
        "2. **Declining Property Value** – If an area consistently shows high crime rates, real estate values might drop, affecting investment and development.\n",
        "3. **Higher Security Costs** – Companies in high-risk areas may need additional security measures, increasing operational expenses.\n",
        "\n",
        "###**Turning Negative Insights into Opportunities**\n",
        "Even insights that seem negative can be **leveraged strategically**—for example, businesses can invest in better security or use predictive analytics to prevent incidents before they occur. Adapting to trends and mitigating risks ensures long-term growth despite initial challenges."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "sns.set(rc={'figure.figsize':(15,10)})\n",
        "sns.set_palette('husl')\n",
        "graph = sns.countplot(data=df, x='YEAR', hue='TYPE')\n",
        "graph.set_title('')"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Line Chart** – If we’re analyzing trends over time (such as incidents per day or month), this chart highlights patterns like seasonal spikes or declines.\n",
        "\n",
        "**Bar Chart** – If comparing different categories (such as crime types or neighborhoods), a bar chart visually distinguishes frequency variations.\n",
        "\n",
        "**Scatter Plot** – When working with geographical data (Latitude and Longitude), a scatter plot helps identify clustering of incidents.\n",
        "\n",
        "**Heatmap** – If analyzing time-based trends (like peak hours for incidents), a heatmap showcases density variations clearly."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Time-Based Patterns** – A line chart may reveal peak crime hours, seasonal trends, or long-term increases/decreases in incidents.\n",
        "\n",
        "**Neighborhood Comparisons** – A bar chart could highlight which areas experience the highest or lowest incidents.\n",
        "\n",
        "**Geospatial Clustering** – A scatter plot using latitude & longitude may show high-risk zones where incidents frequently occur.\n",
        "\n",
        "**Heatmap Trends** – A heatmap focusing on hours or days might pinpoint times when events are most frequent."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Positive Business Impact**\n",
        "1. **Strategic Decision-Making** – Businesses, law enforcement, or policymakers can optimize operations based on crime trends, improving efficiency and safety.\n",
        "2. **Operational Cost Reduction** – Understanding peak incident hours enables smarter resource allocation, cutting unnecessary expenses.\n",
        "3. **Real Estate & Investments** – Identifying low-risk areas can help investors decide where to develop new projects or establish businesses.\n",
        "4. **Enhanced Customer Experience** – Companies in hospitality, retail, or transportation can improve safety measures to increase customer trust.\n",
        "\n",
        "### **Insights That Could Lead to Negative Growth**\n",
        "1. **Reputation Challenges** – If an area has a high crime rate, businesses in that location may struggle to attract customers due to safety concerns.\n",
        "2. **Declining Property Value** – Frequent incidents in specific neighborhoods could lead to lower property prices, impacting real estate markets.\n",
        "3. **Higher Security Costs** – Companies operating in risk-prone areas may need to increase security measures, raising operational expenses.\n",
        "\n",
        "### **Mitigating Risks & Leveraging Insights**\n",
        "Even negative trends can be turned into strategic opportunities—for example:\n",
        "- Businesses can invest in preventive safety measures to improve customer trust.\n",
        "- Government agencies can focus on urban planning & crime prevention in identified hotspots.\n",
        "- Companies can adjust their marketing strategies based on location-based risks.\n"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.rcParams['figure.figsize'] = 12,9\n",
        "labels = df['TYPE'].value_counts().index\n",
        "sizes = df['TYPE'].value_counts().values\n",
        "plt.pie(sizes, labels=labels, autopct='%1.0f%%')\n",
        "plt.title('Crime Type Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer. As this is a Univariate Analysis,we compare the data from one variable or one column \"crime\",so we have considered pie chat"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer. we found that the booking number is higher in theft from vehicle which is 32% than Mischief which is 13%.hence we can say that theft from vehicle has consumption"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Positive Business Impact**:\n",
        "Theft from Vehicle (32%)\n",
        "High demand for vehicle security solutions (alarms, GPS, insurance). Opportunity for safety tech businesses.\n",
        "\n",
        "Mischief (13%) & Break and Enter (12%)\n",
        "Demand for home security systems and neighborhood watch services.\n",
        "\n",
        "Offence Against a Person (10%)\n",
        "Potential for personal safety apps and self-defense products.\n",
        "\n",
        "**Negative Growth Indicators**:\n",
        "Theft of Bicycle (5%) & Vehicle Collision with Injury (4%)\n",
        "May reflect urban safety issues. Could discourage tourism or local travel unless mitigated.\n",
        "\n",
        "Break and Enter Commercial (6%)\n",
        "Might lead to increased business insurance costs or reluctance to open stores in affected areas."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "grouped_by_crime = df['TYPE'].value_counts()\n",
        "grouped_by_crime"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here. As we are analysing crime and adr variables, to know which crime is making more"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theft from Vehicle is the most reported crime (153,932 cases), significantly more than the next highest (Mischief – 63,233).\n",
        "\n",
        "Property-related crimes (e.g., thefts and break-ins) dominate the top categories, showing a pattern.\n",
        "\n",
        "Crimes involving physical harm (like Offence Against a Person or Vehicle Collision with Injury) are fewer in comparison to property crimes.\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Impacts:\n",
        "Insurance companies can use this data to adjust premiums or provide incentives for theft-prevention tools.\n",
        "\n",
        "Local businesses and retailers can use this to invest in targeted security (e.g., vehicle surveillance in parking areas).\n",
        "\n",
        "Urban planners can redesign parking and lighting in high-risk areas.\n",
        "\n",
        "Tech companies can innovate security products (smart alarms, tracking devices).\n",
        "\n",
        "Potential Negative Impact:\n",
        "Highlighting high crime rates in certain areas might discourage investment or tourism if not managed carefully.\n",
        "\n",
        "Businesses in high-theft zones may face higher operational costs for insurance and security."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Check if the DataFrame is loaded and has the 'TYPE' column\n",
        "if df is not None and 'TYPE' in df.columns:\n",
        "    # Get the top 10 most frequent crime types for visualization\n",
        "    top_n = 10\n",
        "    top_crime_types = df['TYPE'].value_counts().nlargest(top_n).index\n",
        "    df_top_crimes = df[df['TYPE'].isin(top_crime_types)]\n",
        "\n",
        "    # Create a countplot for the top crime types\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.countplot(data=df_top_crimes, y='TYPE', order=top_crime_types, palette='viridis')\n",
        "    plt.title(f'Top {top_n} Most Frequent Crime Types')\n",
        "    plt.xlabel('Count')\n",
        "    plt.ylabel('Crime Type')\n",
        "    plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nSkipping Chart - 7 visualization as the dataset was not loaded or 'TYPE' column is missing.\")\n",
        "\n",
        "# The original code was trying to filter based on non-existent columns\n",
        "# not_canceled = df[df['is_canceled']==0]\n",
        "# s1 = not_canceled[not_canceled['total_stay']<15].value_counts()\n",
        "# plt.figure(figsize = (9,7))"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The horizontal bar chart clearly shows the top 10 crime types by frequency, making it easy to compare and identify major crime categories."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Theft from Vehicle** is the most frequent crime.\n",
        "* Property crimes dominate the list over violent ones.\n",
        "* **Vehicle-related crimes** (theft from/ of vehicle, bicycle) are major issues."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Helps insurance, security, and tech firms target key risk areas.\n",
        "* Informs urban planning and law enforcement strategies.\n",
        "* Supports data-driven decision making for crime prevention and resource allocation."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Check if the DataFrame is loaded and has 'HOUR' and 'TYPE' columns\n",
        "if df is not None and 'HOUR' in df.columns and 'TYPE' in df.columns:\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    # Create a pivot table to count crime types by hour\n",
        "    crime_by_hour = df.groupby(['HOUR', 'TYPE']).size().unstack(fill_value=0)\n",
        "    # Plot the crime counts by hour for different crime types\n",
        "    crime_by_hour.plot(kind='line', figsize=(14, 8))\n",
        "    plt.title('Crime Count by Hour and Type')\n",
        "    plt.xlabel('Hour of Day')\n",
        "    plt.ylabel('Number of Incidents')\n",
        "    plt.xticks(range(24)) # Ensure all hours are shown on x-axis\n",
        "    plt.legend(title='Crime Type', bbox_to_anchor=(1.05, 1), loc='upper left') # Move legend outside plot\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nSkipping Chart - 8 visualization as the dataset was not loaded or required columns ('HOUR', 'TYPE') are missing.\")"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line chart shows how different crime types vary by hour of the day, helping identify time-based crime patterns."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theft from Vehicle spikes in the late evening (16–23 hrs).\n",
        "\n",
        "Most crimes are low during early morning hours (1–6 hrs).\n",
        "\n",
        "Mischief and Other Theft also increase in the evening."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps police allocate patrols more effectively by hour.\n",
        "\n",
        "Businesses can adjust security timing for peak crime hours.\n",
        "\n",
        "Enables predictive security measures in high-risk time slots."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Check if the DataFrame is loaded and has the 'MONTH' column\n",
        "if df is not None and 'MONTH' in df.columns:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    # Create a countplot for the distribution of crimes by month\n",
        "    # We can use a specific order for the months (1 to 12) to make the plot chronological\n",
        "    month_order = range(1, 13)\n",
        "    sns.countplot(data=df, x='MONTH', order=month_order, palette='viridis')\n",
        "    plt.title('Total Crime Count by Month')\n",
        "    plt.xlabel('Month')\n",
        "    plt.ylabel('Number of Incidents')\n",
        "    plt.xticks(ticks=month_order, labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "    plt.grid(axis='y', linestyle='--')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nSkipping Chart - 10 visualization as the dataset was not loaded or 'MONTH' column is missing.\")"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This monthly bar chart helps identify seasonal trends in crime across the year for better planning and prevention."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "July has the highest number of incidents.\n",
        "\n",
        "Crime tends to increase in summer months (May–Sep).\n",
        "\n",
        "February has the lowest crime count."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supports seasonal staffing and patrol planning.\n",
        "\n",
        "Retailers, event organizers, and insurers can prepare for peak crime months.\n",
        "\n",
        "Helps cities optimize safety campaigns during high-crime periods."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd # Ensure pandas is imported\n",
        "\n",
        "# Check if the DataFrame is loaded and has the 'Date' column\n",
        "if df is not None and 'Date' in df.columns:\n",
        "    # Ensure 'Date' column is in datetime format\n",
        "    try:\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        # Extract the day of the week (0=Monday, 6=Sunday)\n",
        "        df['Day_of_Week'] = df['Date'].dt.dayofweek\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        # Create a countplot for the distribution of crimes by day of the week\n",
        "        # Order the days from Monday to Sunday\n",
        "        day_order = range(7)\n",
        "        day_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "        sns.countplot(data=df, x='Day_of_Week', order=day_order, palette='viridis')\n",
        "        plt.title('Total Crime Count by Day of the Week')\n",
        "        plt.xlabel('Day of Week')\n",
        "        plt.ylabel('Number of Incidents')\n",
        "        plt.xticks(ticks=day_order, labels=day_labels)\n",
        "        plt.grid(axis='y', linestyle='--')\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing 'Date' column or plotting: {e}\")\n",
        "        print(\"\\nSkipping Chart - 11 visualization.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping Chart - 11 visualization as the dataset was not loaded or 'Date' column is missing.\")"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This day-wise bar chart shows how crime rates vary across the days of the week, helping identify daily patterns."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monday has the highest crime count.\n",
        "\n",
        "Sunday and Monday show peak activity.\n",
        "\n",
        "Mid-week (Tuesday–Friday) has slightly lower crime levels."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enables strategic deployment of police or security on high-crime days.\n",
        "\n",
        "Businesses and public services can strengthen surveillance on weekends and Mondays.\n",
        "\n",
        "Helps plan community outreach or crime prevention campaigns more effectively."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Check if the DataFrame is loaded and has the 'NEIGHBOURHOOD' column\n",
        "if df is not None and 'NEIGHBOURHOOD' in df.columns:\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Get the count of incidents per neighborhood\n",
        "    neighborhood_counts = df['NEIGHBOURHOOD'].value_counts()\n",
        "\n",
        "    # Select the top N neighborhoods (e.g., top 15)\n",
        "    top_n_neighborhoods = 15\n",
        "    if len(neighborhood_counts) > top_n_neighborhoods:\n",
        "        top_neighborhood_list = neighborhood_counts.nlargest(top_n_neighborhoods).index\n",
        "        # Filter the dataframe to include only the top neighborhoods for plotting order\n",
        "        df_top_neighborhoods = df[df['NEIGHBOURHOOD'].isin(top_neighborhood_list)]\n",
        "        # Use a countplot ordered by the top neighborhoods\n",
        "        sns.countplot(data=df_top_neighborhoods, y='NEIGHBOURHOOD', order=top_neighborhood_list, palette='viridis')\n",
        "        plt.title(f'Top {top_n_neighborhoods} Crime Incidents by Neighborhood')\n",
        "    else:\n",
        "        # If fewer than top_n_neighborhoods, just plot all of them ordered by count\n",
        "        sns.countplot(data=df, y='NEIGHBOURHOOD', order=neighborhood_counts.index, palette='viridis')\n",
        "        plt.title('Crime Incidents by Neighborhood')\n",
        "\n",
        "\n",
        "    plt.xlabel('Number of Incidents')\n",
        "    plt.ylabel('Neighborhood')\n",
        "    plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping Chart - 12 visualization as the dataset was not loaded or 'NEIGHBOURHOOD' column is missing.\")"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This horizontal bar chart effectively ranks neighborhoods by total crime incidents, making it easy to compare crime distribution across areas."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Central Business District has the highest number of incidents, followed by West End and Fairview.\n",
        "\n",
        "Some neighborhoods like Killarney and Victoria-Fraserview report significantly lower crime."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps city officials prioritize policing and resource allocation in high-crime areas.\n",
        "\n",
        "Real estate and retail businesses can use this to evaluate safety risks and choose locations wisely."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Check if the DataFrame is loaded and has 'Latitude' and 'Longitude' columns\n",
        "if df is not None and 'Latitude' in df.columns and 'Longitude' in df.columns:\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    # Create a scatter plot of Latitude vs Longitude\n",
        "    # Use alpha to see density where points overlap\n",
        "    sns.scatterplot(data=df, x='Longitude', y='Latitude', alpha=0.5, s=10) # s controls marker size\n",
        "    plt.title('Geographical Distribution of Crime Incidents')\n",
        "    plt.xlabel('Longitude')\n",
        "    plt.ylabel('Latitude')\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout() # Adjust layout\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nSkipping Chart - 13 visualization as the dataset was not loaded or required columns ('Latitude', 'Longitude') are missing.\")"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This scatter plot visualizes the geographical spread of crime using latitude and longitude, helping detect spatial concentration of incidents."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most crimes are clustered around a specific region, likely an urban zone.\n",
        "\n",
        "Some outliers may indicate incorrect or missing geo-data."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enables location-based policing and urban safety planning.\n",
        "\n",
        "Helps businesses and governments optimize surveillance and infrastructure in high-density zones."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if the DataFrame is loaded\n",
        "if df is not None:\n",
        "    # Select only numerical columns for correlation calculation\n",
        "    numerical_df = df.select_dtypes(include=['number'])\n",
        "\n",
        "    # Compute the correlation matrix\n",
        "    # .corr() will automatically handle NaNs by default (pairwise deletion)\n",
        "    correlation_matrix = numerical_df.corr()\n",
        "\n",
        "    # Set up the figure size\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Generate the heatmap\n",
        "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\n",
        "\n",
        "    # Add title\n",
        "    plt.title(\"Feature Correlation Heatmap\")\n",
        "\n",
        "    # Show the heatmap\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nSkipping Chart - 14 visualization as the dataset was not loaded.\")"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This correlation heatmap helps understand relationships between features, which is crucial for feature selection and improving model accuracy."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "X, Y, and Latitude are highly correlated, indicating redundancy.\n",
        "\n",
        "Other features like HOUR, MINUTE, DAY, etc., show low correlation, meaning they contribute unique patterns."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps in removing redundant features, reducing model complexity.\n",
        "\n",
        "Enhances prediction performance and data efficiency for time-based crime forecasting."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 16 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Check if the DataFrame is loaded and has 'YEAR' and 'TYPE' columns\n",
        "if df is not None and 'YEAR' in df.columns and 'TYPE' in df.columns:\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    # Get the total count of incidents per year for each crime type\n",
        "    # We'll focus on the top N crime types to keep the plot manageable\n",
        "    top_n_types = 5 # Adjust N as needed\n",
        "    top_crime_types = df['TYPE'].value_counts().nlargest(top_n_types).index\n",
        "\n",
        "    # Filter the DataFrame to include only the top crime types\n",
        "    df_top_types = df[df['TYPE'].isin(top_crime_types)]\n",
        "\n",
        "    # Group by YEAR and TYPE and count the occurrences\n",
        "    crime_trend_by_type = df_top_types.groupby(['YEAR', 'TYPE']).size().unstack(fill_value=0)\n",
        "\n",
        "    # Plot the time series for each top crime type\n",
        "    crime_trend_by_type.plot(kind='line', figsize=(15, 8))\n",
        "\n",
        "    plt.title(f'Annual Crime Trends for Top {top_n_types} Crime Types')\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Number of Incidents')\n",
        "    plt.xticks(crime_trend_by_type.index) # Ensure all years are shown on x-axis\n",
        "    plt.legend(title='Crime Type', bbox_to_anchor=(1.05, 1), loc='upper left') # Move legend outside plot\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout() # Adjust layout\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nSkipping Chart - 16 visualization as the dataset was not loaded or required columns ('YEAR', 'TYPE') are missing.\")"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line chart effectively shows yearly trends for the top 5 crime types over time, highlighting long-term patterns."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall decline in the most frequent crime type from 1999 to 2007.\n",
        "\n",
        "Some crimes remained steady or slightly increased, showing diverging patterns."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 15 visualization code\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# import sys # No longer needed for this error handling approach\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming the file path is correct based on your previous code\n",
        "# try:\n",
        "#     df = pd.read_excel('/content/drive/MyDrive/Train.xlsx') # Loading again might not be necessary if df is already loaded\n",
        "\n",
        "# Check if the DataFrame is loaded\n",
        "if df is not None:\n",
        "    # Select a subset of numerical columns for the pair plot\n",
        "    # Choose columns that are most likely to have interesting relationships\n",
        "    selected_columns = [\"YEAR\", \"MONTH\", \"HOUR\", \"Latitude\", \"Longitude\"]\n",
        "\n",
        "    # Check if selected_columns exist in the dataframe before subsetting\n",
        "    missing_cols = [col for col in selected_columns if col not in df.columns]\n",
        "    if missing_cols:\n",
        "        print(f\"Skipping Chart - 15 visualization: Missing columns for Pair Plot: {missing_cols}\")\n",
        "    else:\n",
        "        # Create a subset DataFrame with the selected numerical columns\n",
        "        df_subset = df[selected_columns]\n",
        "\n",
        "        # Create the pair plot\n",
        "        # This can take time depending on the number of rows and columns selected\n",
        "        print(\"Generating Pair Plot (this might take a moment)...\")\n",
        "        sns.pairplot(df_subset, palette='viridis', diag_kind='kde') # diag_kind='kde' for smoother density plots on diagonal\n",
        "\n",
        "        # Add a title to the overall figure (optional, often added manually after creation)\n",
        "        # plt.suptitle('Pair Plot of Selected Numerical Features', y=1.02) # Adjust y position as needed\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping Chart - 15 visualization as the dataset was not loaded.\")"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This pair plot is chosen to visually explore relationships between key features like year, month, hour, and geographic coordinates."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crime incidents peak around certain hours of the day.\n",
        "\n",
        "Clustering of incidents around specific latitudes and longitudes, indicating hotspot areas."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement\n",
        "1. **Hypothesis 1:**\n",
        "   *The average crime rate in Urban areas is higher than in Rural areas.*\n",
        "2. **Hypothesis 2:**\n",
        "   *There is no association between the type of crime and the region (North, South, East, West).*\n",
        "3. **Hypothesis 3:**\n",
        "   *The average monthly sales before and after a marketing campaign are different.*\n",
        "These hypotheses cover:\n",
        "* Comparing means (Hypothesis 1 & 3) — t-test or similar\n",
        "* Testing association between categorical variables (Hypothesis 2) — Chi-square test\n",
        "### Next Steps:\n",
        "\n",
        "* For **Hypothesis 1 and 3**: Use **t-tests** (two-sample or paired).\n",
        "* For **Hypothesis 2**: Use **Chi-square test of independence**."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis 1: Crime Rate in Urban vs Rural Areas\n",
        "Null Hypothesis (H0):\n",
        "The average crime rate in Urban areas is equal to the average crime rate in Rural areas.\n",
        "\n",
        "𝐻0: 𝜇𝑈𝑟𝑏𝑎𝑛=𝜇𝑅𝑢𝑟𝑎𝑙H 0​ :μ Urban​=μ Rural\n",
        "\n",
        "\n",
        "Alternative Hypothesis (H1):\n",
        "The average crime rate in Urban areas is higher than in Rural areas.\n",
        "𝐻1 : 𝜇𝑈𝑟𝑏𝑎𝑛 > 𝜇𝑅𝑢𝑟𝑎𝑙H 1​ :μ Urban​ > μ Rural​\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Check if the DataFrame is loaded and has the required columns\n",
        "if df is not None and 'TYPE' in df.columns and 'NEIGHBOURHOOD' in df.columns:\n",
        "    # Create a contingency table of 'TYPE' and 'NEIGHBOURHOOD'\n",
        "    # The chi-squared test works best when expected frequencies are not too small.\n",
        "    # Including all neighborhoods and crime types might lead to very large tables with small counts.\n",
        "    # For demonstration, let's use the full data, but be aware of potential warnings or less reliable results\n",
        "    # with sparse tables.\n",
        "    contingency_table = pd.crosstab(df['TYPE'], df['NEIGHBOURHOOD'])\n",
        "\n",
        "    # Perform the Chi-Squared Test for Independence\n",
        "    # chi2: the test statistic\n",
        "    # p: the p-value\n",
        "    # dof: degrees of freedom\n",
        "    # expected: the expected frequencies, based on the assumption of independence\n",
        "    try:\n",
        "        chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "        # Print the results\n",
        "        print(\"Chi-Squared Test for Independence between Crime Type and Neighborhood\")\n",
        "        print(f\"Chi-Squared Statistic: {chi2:.4f}\")\n",
        "        print(f\"P-value: {p:.4f}\")\n",
        "        print(f\"Degrees of Freedom: {dof}\")\n",
        "\n",
        "        # Interpret the P-value\n",
        "        alpha = 0.05 # Set significance level (commonly 0.05)\n",
        "        if p < alpha:\n",
        "            print(f\"\\nWith a P-value ({p:.4f}) less than the significance level ({alpha}), we reject the null hypothesis.\")\n",
        "            print(\"There is statistically significant evidence to suggest that Crime Type and Neighborhood are NOT independent.\")\n",
        "        else:\n",
        "            print(f\"\\nWith a P-value ({p:.4f}) greater than or equal to the significance level ({alpha}), we fail to reject the null hypothesis.\")\n",
        "            print(\"There is not enough statistically significant evidence to suggest that Crime Type and Neighborhood are NOT independent.\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"\\nCould not perform Chi-Squared test. Error: {e}\")\n",
        "        print(\"This might be due to zero rows/columns in the contingency table or very small expected counts.\")\n",
        "        print(\"Consider filtering data or combining categories if needed for the test.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping Statistical Test: Dataset not loaded or required columns ('TYPE', 'NEIGHBOURHOOD') are missing.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Test Used: Independent Samples t-test (one-tailed)\n",
        "\n",
        "Why: Comparing means between two independent groups\n",
        "\n",
        "How P-value is obtained:\n",
        "The scipy.stats.ttest_ind() function is used in Python. The returned p-value tells us the probability of observing the data if the null hypothesis (equal means) were true."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are comparing means between two independent groups (Urban vs Rural).\n",
        "\n",
        "The data is continuous (crime rate).\n",
        "\n",
        "The groups are independent (urban and rural are separate populations).\n",
        "\n",
        "We are checking whether one mean is greater than the other, so it’s a one-tailed test."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0):\n",
        "There is no association between the type of crime and the region. They are independent.\n",
        "𝐻\n",
        "0\n",
        ":\n",
        "Crime Type\n",
        "⊥\n",
        "Region\n",
        "H\n",
        "0\n",
        "​\n",
        " :Crime Type⊥Region\n",
        "\n",
        "Alternative Hypothesis (H1):\n",
        "There is an association between the type of crime and the region.\n",
        "𝐻\n",
        "1\n",
        ":\n",
        "Crime Type\n",
        "⊥̸\n",
        "Region\n",
        "H\n",
        "1\n",
        "​\n",
        " :Crime Type\n",
        "\n",
        "⊥RegionAnswer Here."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test 2 to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Check if the DataFrame is loaded and has the required columns\n",
        "if df is not None and 'Latitude' in df.columns and 'Longitude' in df.columns:\n",
        "    # Select the two numerical columns\n",
        "    latitudes = df['Latitude']\n",
        "    longitudes = df['Longitude']\n",
        "\n",
        "    # Drop rows with missing values in either column for the correlation calculation\n",
        "    # pearsonr handles NaNs by dropping pairs, but explicitly dropping can be cleaner\n",
        "    combined = pd.DataFrame({'Latitude': latitudes, 'Longitude': longitudes}).dropna()\n",
        "\n",
        "    # Ensure there is enough data left after dropping NaNs (at least 2 points)\n",
        "    if len(combined) < 2:\n",
        "        print(\"\\nSkipping Statistical Test 2: Not enough valid data points (non-NaN Latitude/Longitude pairs) to calculate correlation.\")\n",
        "    else:\n",
        "        # Perform the Pearson correlation test\n",
        "        # correlation_coefficient: the Pearson correlation coefficient\n",
        "        # p_value: the p-value\n",
        "        try:\n",
        "            correlation_coefficient, p_value = pearsonr(combined['Latitude'], combined['Longitude'])\n",
        "\n",
        "            # Print the results\n",
        "            print(\"Pearson Correlation Test between Latitude and Longitude\")\n",
        "            print(f\"Pearson Correlation Coefficient: {correlation_coefficient:.4f}\")\n",
        "            print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "            # Interpret the P-value and correlation coefficient\n",
        "            alpha = 0.05 # Set significance level\n",
        "            print(\"\\nInterpretation:\")\n",
        "            if p_value < alpha:\n",
        "                print(f\"With a P-value ({p_value:.4f}) less than the significance level ({alpha}), we reject the null hypothesis.\")\n",
        "                print(\"There is statistically significant evidence of a linear relationship between Latitude and Longitude.\")\n",
        "                print(f\"The correlation coefficient ({correlation_coefficient:.4f}) indicates the strength and direction of this linear relationship.\")\n",
        "            else:\n",
        "                print(f\"With a P-value ({p_value:.4f}) greater than or equal to the significance level ({alpha}), we fail to reject the null hypothesis.\")\n",
        "                print(\"There is not enough statistically significant evidence to suggest a linear relationship between Latitude and Longitude.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nAn unexpected error occurred during the Pearson correlation test: {e}\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping Statistical Test 2: Dataset not loaded or required columns ('Latitude', 'Longitude') are missing.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Test Used: Chi-Square Test of Independence\n",
        "\n",
        "Why: Testing association between two categorical variables\n",
        "\n",
        "How P-value is obtained:\n",
        "The scipy.stats.chi2_contingency() function is used in Python. It calculates a Chi-square statistic and p-value, which tells us how likely the observed distribution is under the assumption of independence."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both variables (crime type, region) are categorical.\n",
        "\n",
        "We want to see if there is an association or dependency between them.\n",
        "\n",
        "The chi-square test is the standard test to determine if distributions of categorical variables differ from each other."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0):\n",
        "The average monthly sales before and after the marketing campaign are equal.\n",
        "𝐻\n",
        "0\n",
        ":\n",
        "𝜇\n",
        "𝐵\n",
        "𝑒\n",
        "𝑓\n",
        "𝑜\n",
        "𝑟\n",
        "𝑒\n",
        "=\n",
        "𝜇\n",
        "𝐴\n",
        "𝑓\n",
        "𝑡\n",
        "𝑒\n",
        "𝑟\n",
        "H\n",
        "0\n",
        "​\n",
        " :μ\n",
        "Before\n",
        "​\n",
        " =μ\n",
        "After\n",
        "​\n",
        "\n",
        "\n",
        "Alternative Hypothesis (H1):\n",
        "The average monthly sales before and after the marketing campaign are different.\n",
        "𝐻\n",
        "1\n",
        ":\n",
        "𝜇\n",
        "𝐵\n",
        "𝑒\n",
        "𝑓\n",
        "𝑜\n",
        "𝑟\n",
        "𝑒\n",
        "≠\n",
        "𝜇\n",
        "𝐴\n",
        "𝑓\n",
        "𝑡\n",
        "𝑒\n",
        "𝑟\n",
        "H\n",
        "1\n",
        "​\n",
        " :μ\n",
        "Before\n",
        "​\n",
        "\n",
        "\n",
        "=μ\n",
        "After\n",
        "​\n"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test 3 to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Check if the DataFrame is loaded and has the required columns\n",
        "if df is not None and 'TYPE' in df.columns and 'Latitude' in df.columns:\n",
        "    # Get the list of unique crime types\n",
        "    crime_types = df['TYPE'].unique()\n",
        "\n",
        "    # Create a list of Latitude arrays, one for each crime type\n",
        "    # Filter out potential NaN values in Latitude\n",
        "    latitude_by_crime_type = [df[df['TYPE'] == crime_type]['Latitude'].dropna().values for crime_type in crime_types]\n",
        "\n",
        "    # Filter out crime types that might have resulted in empty arrays after dropping NaNs\n",
        "    latitude_by_crime_type = [arr for arr in latitude_by_crime_type if len(arr) > 0]\n",
        "\n",
        "    # Ensure there is more than one group (crime type) with data to compare\n",
        "    if len(latitude_by_crime_type) < 2:\n",
        "         print(\"\\nSkipping Statistical Test 3: Not enough distinct crime types with Latitude data to perform ANOVA.\")\n",
        "    else:\n",
        "        # Perform the One-Way ANOVA test\n",
        "        # f_statistic: the test statistic\n",
        "        # p_value: the p-value\n",
        "        try:\n",
        "            f_statistic, p_value = f_oneway(*latitude_by_crime_type)\n",
        "\n",
        "            # Print the results\n",
        "            print(\"One-Way ANOVA Test for Mean Latitude across different Crime Types\")\n",
        "            print(f\"F-Statistic: {f_statistic:.4f}\")\n",
        "            print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "            # Interpret the P-value\n",
        "            alpha = 0.05 # Set significance level\n",
        "            if p_value < alpha:\n",
        "                print(f\"\\nWith a P-value ({p_value:.4f}) less than the significance level ({alpha}), we reject the null hypothesis.\")\n",
        "                print(\"There is statistically significant evidence to suggest that the mean Latitude is NOT the same for all crime types.\")\n",
        "            else:\n",
        "                print(f\"\\nWith a P-value ({p_value:.4f}) greater than or equal to the significance level ({alpha}), we fail to reject the null hypothesis.\")\n",
        "                print(\"There is not enough statistically significant evidence to suggest that the mean Latitude is different across crime types.\")\n",
        "\n",
        "        except ValueError as e:\n",
        "             print(f\"\\nCould not perform ANOVA test. Error: {e}\")\n",
        "             print(\"This might happen if one or more groups have zero variance or if there are other data issues.\")\n",
        "        except Exception as e:\n",
        "             print(f\"\\nAn unexpected error occurred during the ANOVA test: {e}\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping Statistical Test 3: Dataset not loaded or required columns ('TYPE', 'Latitude') are missing.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Test Used: Paired Samples t-test (two-tailed)\n",
        "\n",
        "Why: Comparing means of related groups\n",
        "\n",
        "How P-value is obtained:\n",
        "The scipy.stats.ttest_rel() function is used. It returns a p-value indicating whether the mean difference between the paired observations is statistically significant."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are comparing the means of two related (paired) datasets (same stores/salespeople before and after campaign).\n",
        "\n",
        "The data is continuous (sales figures).\n",
        "\n",
        "The samples are not independent — we’re tracking the same subjects before and after.\n",
        "\n",
        "Since we’re checking for any difference (not direction-specific), it’s a two-tailed test."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "# Display the count of missing values per column again\n",
        "print(\"Missing values before handling:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Display the percentage of missing values\n",
        "print(\"Percentage of missing values before handling:\")\n",
        "print(round((df.isnull().sum()/df.shape[0])*100))\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# --- Imputation Strategy Examples ---\n",
        "\n",
        "# Example 1: Impute numerical columns (like X, Y, Latitude, Longitude) with the Mean or Median\n",
        "# Choose Median if the data is skewed or contains outliers\n",
        "# Choose Mean if the data is approximately symmetrically distributed\n",
        "numerical_cols_to_impute_median = ['X', 'Y', 'Latitude', 'Longitude'] # Adjust based on your data and needs\n",
        "for col in numerical_cols_to_impute_median:\n",
        "    if col in df.columns and df[col].isnull().sum() > 0:\n",
        "        median_val = df[col].median()\n",
        "        df[col].fillna(median_val, inplace=True)\n",
        "        print(f\"Imputed missing values in '{col}' with the Median ({median_val:.4f}).\")\n",
        "\n",
        "# Example 2: Impute categorical columns (like HUNDRED_BLOCK, NEIGHBOURHOOD) with the Mode\n",
        "categorical_cols_to_impute_mode = ['HUNDRED_BLOCK', 'NEIGHBOURHOOD'] # Adjust based on your data and needs\n",
        "for col in categorical_cols_to_impute_mode:\n",
        "     if col in df.columns and df[col].isnull().sum() > 0:\n",
        "        # Calculate mode, handle potential multiple modes by taking the first\n",
        "        mode_val = df[col].mode()[0]\n",
        "        df[col].fillna(mode_val, inplace=True)\n",
        "        print(f\"Imputed missing values in '{col}' with the Mode ('{mode_val}').\")\n",
        "\n",
        "# Example 3: Impute with a constant value (e.g., 'Unknown' for categorical)\n",
        "# This is an alternative to mode imputation for categorical features\n",
        "categorical_cols_to_impute_constant = [] # Add columns here if you prefer constant imputation\n",
        "constant_value = 'Unknown'\n",
        "for col in categorical_cols_to_impute_constant:\n",
        "    if col in df.columns and df[col].isnull().sum() > 0:\n",
        "        df[col].fillna(constant_value, inplace=True)\n",
        "        print(f\"Imputed missing values in '{col}' with the constant value '{constant_value}'.\")\n",
        "\n",
        "\n",
        "# --- Verification ---\n",
        "# Display missing values again after imputation to confirm\n",
        "print(\"\\nMissing values after handling:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments 2 (Capping/Winsorizing using IQR)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Choose a numerical column for outlier treatment\n",
        "# 'Latitude' and 'Longitude' are good candidates for demonstrating spatial outliers\n",
        "column_to_treat = 'Latitude' # Or 'Longitude', 'X', 'Y', etc.\n",
        "\n",
        "# Check if the DataFrame is loaded and the chosen column exists and is numerical\n",
        "if df is not None and column_to_treat in df.columns and pd.api.types.is_numeric_dtype(df[column_to_treat]):\n",
        "\n",
        "    print(f\"Analyzing and treating outliers for '{column_to_treat}'...\")\n",
        "\n",
        "    # --- 1. Visualize Outliers (Optional but Recommended) ---\n",
        "    plt.figure(figsize=(10, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.boxplot(x=df[column_to_treat].dropna()) # Drop NaNs for boxplot\n",
        "    plt.title(f'Box Plot of {column_to_treat} (Before Treatment)')\n",
        "    plt.xlabel(column_to_treat)\n",
        "\n",
        "    # --- 2. Detect Outliers using IQR ---\n",
        "    Q1 = df[column_to_treat].quantile(0.25)\n",
        "    Q3 = df[column_to_treat].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    print(f\"\\nIQR Calculation for '{column_to_treat}':\")\n",
        "    print(f\"Q1 (25th percentile): {Q1:.4f}\")\n",
        "    print(f\"Q3 (75th percentile): {Q3:.4f}\")\n",
        "    print(f\"IQR: {IQR:.4f}\")\n",
        "    print(f\"Lower Bound (Q1 - 1.5*IQR): {lower_bound:.4f}\")\n",
        "    print(f\"Upper Bound (Q3 + 1.5*IQR): {upper_bound:.4f}\")\n",
        "\n",
        "    # Identify outliers\n",
        "    outliers = df[(df[column_to_treat] < lower_bound) | (df[column_to_treat] > upper_bound)]\n",
        "    print(f\"\\nNumber of outliers detected: {len(outliers)}\")\n",
        "    print(f\"Percentage of outliers: {(len(outliers) / len(df)) * 100:.2f}%\")\n",
        "\n",
        "    # --- 3. Treat Outliers by Capping (Winsorizing) ---\n",
        "    # Make a copy to avoid modifying the original DataFrame slice directly if needed for other analyses\n",
        "    df_treated = df.copy()\n",
        "\n",
        "    # Cap values below the lower bound to the lower bound\n",
        "    df_treated[column_to_treat] = np.where(\n",
        "        df_treated[column_to_treat] < lower_bound,\n",
        "        lower_bound,\n",
        "        df_treated[column_to_treat]\n",
        "    )\n",
        "\n",
        "    # Cap values above the upper bound to the upper bound\n",
        "    df_treated[column_to_treat] = np.where(\n",
        "        df_treated[column_to_treat] > upper_bound,\n",
        "        upper_bound,\n",
        "        df_treated[column_to_treat]\n",
        "    )\n",
        "\n",
        "    print(f\"\\nOutliers in '{column_to_treat}' have been capped using the IQR method.\")\n",
        "\n",
        "\n",
        "    # --- 4. Visualize After Treatment (Optional but Recommended) ---\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.boxplot(x=df_treated[column_to_treat].dropna()) # Drop NaNs for boxplot\n",
        "    plt.title(f'Box Plot of {column_to_treat} (After Capping)')\n",
        "    plt.xlabel(column_to_treat)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # You can now use df_treated for subsequent analysis where outliers are treated\n",
        "\n",
        "else:\n",
        "    print(f\"\\nSkipping Outlier Handling: Dataset not loaded, column '{column_to_treat}' not found, or column is not numerical.\")"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Check the number of unique values in categorical columns\n",
        "print(\"Unique values count for potential categorical columns:\")\n",
        "categorical_cols = ['TYPE', 'HUNDRED_BLOCK', 'NEIGHBOURHOOD']\n",
        "for col in categorical_cols:\n",
        "    if col in df.columns:\n",
        "        print(f\"'{col}': {df[col].nunique()} unique values\")\n",
        "    else:\n",
        "        print(f\"'{col}' not found in DataFrame.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# Check if the DataFrame is loaded\n",
        "if df is not None:\n",
        "\n",
        "    # List of columns to One-Hot Encode\n",
        "    # Let's start with 'TYPE'. If 'HUNDRED_BLOCK' or 'NEIGHBOURHOOD' have a huge number of unique values,\n",
        "    # One-Hot Encoding them might not be feasible due to memory constraints and dimensionality.\n",
        "    cols_to_onehot = ['TYPE']\n",
        "\n",
        "    # Check if the columns exist in the DataFrame\n",
        "    cols_exist = [col for col in cols_to_onehot if col in df.columns]\n",
        "    cols_missing = [col for col in cols_to_onehot if col not in df.columns]\n",
        "\n",
        "    if cols_missing:\n",
        "        print(f\"Skipping One-Hot Encoding for missing columns: {cols_missing}\")\n",
        "\n",
        "    if cols_exist:\n",
        "        print(f\"Performing One-Hot Encoding for columns: {cols_exist}\")\n",
        "\n",
        "        # Perform One-Hot Encoding\n",
        "        # drop_first=True is often used to avoid multicollinearity (dummy variable trap)\n",
        "        df_encoded = pd.get_dummies(df, columns=cols_exist, drop_first=True)\n",
        "\n",
        "        print(\"\\nDataFrame shape after One-Hot Encoding:\")\n",
        "        print(df_encoded.shape)\n",
        "        print(\"\\nFirst 5 rows of the DataFrame after encoding:\")\n",
        "        print(df_encoded.head())\n",
        "\n",
        "    else:\n",
        "        print(\"\\nNo columns selected for One-Hot Encoding were found in the DataFrame.\")\n",
        "\n",
        "\n",
        "    # --- Consideration for High Cardinality Columns ---\n",
        "    print(\"\\nConsiderations for columns with high cardinality ('HUNDRED_BLOCK', 'NEIGHBOURHOOD'):\")\n",
        "    print(\"If these columns have a large number of unique values, One-Hot Encoding them directly\")\n",
        "    print(\"can create a very wide DataFrame, potentially causing memory issues and affecting model performance.\")\n",
        "    print(\"Alternative strategies for high cardinality categorical features include:\")\n",
        "    print(\"- Frequency Encoding\")\n",
        "    print(\"- Target Encoding\")\n",
        "    print(\"- Grouping rare categories before encoding\")\n",
        "    print(\"- Using models that can handle categorical features directly (e.g., LightGBM, CatBoost)\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping Encoding: Dataset not loaded.\")\n",
        "\n",
        "# Now you can use df_encoded for analysis and modeling"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction (Illustrative Code - Likely Not Needed for your current dataset)\n",
        "\n",
        "# Define a dictionary of common contractions\n",
        "contractions_dict = {\n",
        "    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\",\n",
        "    \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\n",
        "    \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
        "    \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\n",
        "    \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\",\n",
        "    \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n",
        "    \"I'm\": \"I am\", \"I've\": \"I have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n",
        "    \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\",\n",
        "    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\",\n",
        "    \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\",\n",
        "    \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n",
        "    \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "    \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
        "    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
        "    \"so've\": \"so have\", \"so's\": \"so is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
        "    \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\n",
        "    \"there's\": \"there is\", \"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
        "    \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n",
        "    \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
        "    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\",\n",
        "    \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "    \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
        "    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n",
        "    \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "    \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n",
        "    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "    \"you're\": \"you are\", \"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    \"\"\"Expands contractions in a text string.\"\"\"\n",
        "    if isinstance(text, str): # Only process strings\n",
        "        # Use a regular expression to find contractions (case-insensitive)\n",
        "        contractions_pattern = r'\\b({})\\b'.format('|'.join(contractions_dict.keys()))\n",
        "        def replacer(match):\n",
        "            # Replace with the expanded form, preserving case if needed (simplified here)\n",
        "            return contractions_dict[match.group(0).lower()]\n",
        "        expanded_text = re.sub(contractions_pattern, replacer, text, flags=re.IGNORECASE)\n",
        "        return expanded_text\n",
        "    else:\n",
        "        return text # Return non-string data as is\n",
        "\n",
        "# Example Usage (assuming you have a text column, e.g., 'Description'):\n",
        "# import re # You would need to import re for regex\n",
        "\n",
        "# if df is not None and 'Description' in df.columns: # Replace 'Description' with your actual text column name\n",
        "#     print(\"Expanding contractions in 'Description' column...\")\n",
        "#     df['Description_expanded'] = df['Description'].apply(lambda x: expand_contractions(x, contractions_dict))\n",
        "#     print(\"Contraction expansion complete.\")\n",
        "#     # You might want to compare original vs expanded text for a few rows\n",
        "#     # print(df[['Description', 'Description_expanded']].head())\n",
        "# else:\n",
        "#     print(\"\\nSkipping Contraction Expansion: Dataset not loaded or no 'Description' column found.\")\n",
        "\n",
        "print(\"Contraction expansion is likely not needed for the current columns in your dataset.\")"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Check if the DataFrame is loaded\n",
        "if df is not None:\n",
        "\n",
        "    # List of columns to convert to lowercase\n",
        "    # These are typically the categorical columns that contain text\n",
        "    cols_to_lowercase = ['TYPE', 'HUNDRED_BLOCK', 'NEIGHBOURHOOD'] # Add other text columns if applicable\n",
        "\n",
        "    print(\"Applying lowercasing to relevant columns...\")\n",
        "\n",
        "    for col in cols_to_lowercase:\n",
        "        if col in df.columns:\n",
        "            # Check if the column is of object (string) type before applying .str.lower()\n",
        "            if pd.api.types.is_object_dtype(df[col]):\n",
        "                # Apply the lower() method to the string representation of each element\n",
        "                # Use .astype(str) just in case there are non-string types that pandas didn't infer\n",
        "                df[col] = df[col].astype(str).str.lower()\n",
        "                print(f\"Successfully lowercased '{col}'.\")\n",
        "            else:\n",
        "                 print(f\"Column '{col}' is not of object type, skipping lowercasing.\")\n",
        "        else:\n",
        "            print(f\"Column '{col}' not found in DataFrame, skipping lowercasing.\")\n",
        "\n",
        "    print(\"\\nFirst 5 rows of the DataFrame after lowercasing:\")\n",
        "    print(df.head())\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping Lower Casing: Dataset not loaded.\")\n",
        "\n",
        "# Now, values like 'THEFT FROM VEHICLE' and 'theft from vehicle' will be standardized to 'theft from vehicle'"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "\n",
        "import pandas as pd\n",
        "import string # Import the string module to get punctuation characters\n",
        "\n",
        "# Check if the DataFrame is loaded\n",
        "if df is not None:\n",
        "\n",
        "    # List of columns to remove punctuation from\n",
        "    # These are typically the text/categorical columns\n",
        "    cols_to_clean_punctuation = ['TYPE', 'HUNDRED_BLOCK', 'NEIGHBOURHOOD'] # Add other text columns if applicable\n",
        "\n",
        "    # Define the set of punctuation characters to remove\n",
        "    punctuations_to_remove = string.punctuation\n",
        "\n",
        "    # Create a translation table to remove punctuation efficiently\n",
        "    translator = str.maketrans('', '', punctuations_to_remove)\n",
        "\n",
        "    print(\"Removing punctuation from relevant columns...\")\n",
        "\n",
        "    for col in cols_to_clean_punctuation:\n",
        "        if col in df.columns:\n",
        "            # Check if the column is of object (string) type\n",
        "            if pd.api.types.is_object_dtype(df[col]):\n",
        "                # Apply the translation table to remove punctuation\n",
        "                # Use .astype(str) to ensure all elements are strings before applying translate\n",
        "                df[col] = df[col].astype(str).str.translate(translator)\n",
        "                print(f\"Successfully removed punctuation from '{col}'.\")\n",
        "            else:\n",
        "                 print(f\"Column '{col}' is not of object type, skipping punctuation removal.\")\n",
        "        else:\n",
        "            print(f\"Column '{col}' not found in DataFrame, skipping punctuation removal.\")\n",
        "\n",
        "    print(\"\\nFirst 5 rows of the DataFrame after removing punctuation:\")\n",
        "    print(df.head())\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping Punctuation Removal: Dataset not loaded.\")\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "\n",
        "import pandas as pd\n",
        "import re # Import the regular expression module\n",
        "\n",
        "# Check if the DataFrame is loaded\n",
        "if df is not None:\n",
        "\n",
        "    # List of columns to clean text from\n",
        "    # These are typically the text/categorical columns\n",
        "    cols_to_clean_text = ['TYPE', 'HUNDRED_BLOCK', 'NEIGHBOURHOOD'] # Add other text columns if applicable\n",
        "\n",
        "    # Regex pattern to find URLs\n",
        "    # This is a basic pattern and might not catch all complex URLs\n",
        "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "\n",
        "    # Regex pattern to find words containing digits\n",
        "    # \\b asserts a word boundary\n",
        "    # \\w* matches zero or more word characters (letters, digits, underscore)\n",
        "    # \\d+ matches one or more digits\n",
        "    # \\w* matches zero or more word characters after the digits\n",
        "    # This pattern finds entire words that have digits in them\n",
        "    word_with_digits_pattern = r'\\b\\w*\\d\\w*\\b'\n",
        "\n",
        "    print(\"Removing URLs and words/digits containing digits from relevant columns...\")\n",
        "\n",
        "    for col in cols_to_clean_text:\n",
        "        if col in df.columns:\n",
        "            # Check if the column is of object (string) type\n",
        "            if pd.api.types.is_object_dtype(df[col]):\n",
        "                # Ensure all elements are strings to apply regex methods\n",
        "                df[col] = df[col].astype(str)\n",
        "\n",
        "                # Remove URLs\n",
        "                df[col] = df[col].str.replace(url_pattern, '', regex=True)\n",
        "                print(f\"Successfully removed URLs from '{col}'.\")\n",
        "\n",
        "                # Remove words containing digits\n",
        "                df[col] = df[col].str.replace(word_with_digits_pattern, '', regex=True)\n",
        "                print(f\"Successfully removed words/digits containing digits from '{col}'.\")\n",
        "            else:\n",
        "                 print(f\"Column '{col}' is not of object type, skipping URL/digit word removal.\")\n",
        "        else:\n",
        "            print(f\"Column '{col}' not found in DataFrame, skipping URL/digit word removal.\")\n",
        "\n",
        "    # After removing words/digits, you might be left with extra whitespace\n",
        "    # It's a good practice to remove leading/trailing whitespace and collapse multiple spaces\n",
        "    for col in cols_to_clean_text:\n",
        "        if col in df.columns and pd.api.types.is_object_dtype(df[col]):\n",
        "             df[col] = df[col].str.strip().str.replace(r'\\s+', ' ', regex=True) # Remove extra spaces\n",
        "             # print(f\"Cleaned up whitespace in '{col}'.\") # Optional confirmation\n",
        "\n",
        "    print(\"\\nFirst 5 rows of the DataFrame after text cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping URL/Digit word Removal: Dataset not loaded.\")"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re # Needed for splitting text into words\n",
        "\n",
        "# --- Download NLTK stopwords if not already downloaded ---\n",
        "try:\n",
        "    stopwords = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    print(\"NLTK stopwords not found. Downloading...\")\n",
        "    nltk.download('stopwords')\n",
        "    stopwords = set(stopwords.words('english'))\n",
        "    print(\"NLTK stopwords downloaded.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Check if the DataFrame is loaded\n",
        "if df is not None:\n",
        "\n",
        "    # List of columns to remove stopwords from\n",
        "    # These are typically the text/categorical columns\n",
        "    cols_to_clean_stopwords = ['TYPE', 'HUNDRED_BLOCK', 'NEIGHBOURHOOD'] # Add other text columns if applicable\n",
        "\n",
        "    print(\"Removing stopwords from relevant columns...\")\n",
        "\n",
        "    # Function to remove stopwords from a single string\n",
        "    def remove_stopwords_from_text(text):\n",
        "        if isinstance(text, str):\n",
        "            # Split the text into words, convert to lowercase (important!), and remove non-alphanumeric\n",
        "            words = re.findall(r'\\b\\w+\\b', text.lower()) # Using regex to find words\n",
        "            # Filter out stopwords\n",
        "            filtered_words = [word for word in words if word not in stopwords]\n",
        "            # Join the words back into a string\n",
        "            return ' '.join(filtered_words)\n",
        "        else:\n",
        "            return text # Return non-string data as is\n",
        "\n",
        "    for col in cols_to_clean_stopwords:\n",
        "        if col in df.columns:\n",
        "            # Check if the column is of object (string) type\n",
        "            if pd.api.types.is_object_dtype(df[col]):\n",
        "                # Apply the function to each element in the column\n",
        "                df[col] = df[col].apply(remove_stopwords_from_text)\n",
        "                print(f\"Successfully removed stopwords from '{col}'.\")\n",
        "            else:\n",
        "                 print(f\"Column '{col}' is not of object type, skipping stopword removal.\")\n",
        "        else:\n",
        "            print(f\"Column '{col}' not found in DataFrame, skipping stopword removal.\")\n",
        "\n",
        "    print(\"\\nFirst 5 rows of the DataFrame after removing stopwords:\")\n",
        "    print(df.head())\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping Stopword Removal: Dataset not loaded.\")"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "\n",
        "import pandas as pd\n",
        "import re # Import the regular expression module\n",
        "\n",
        "# Check if the DataFrame is loaded\n",
        "if df is not None:\n",
        "\n",
        "    # List of columns to remove whitespace from\n",
        "    # These are typically the text/categorical columns\n",
        "    cols_to_clean_whitespace = ['TYPE', 'HUNDRED_BLOCK', 'NEIGHBOURHOOD'] # Add other text columns if applicable\n",
        "\n",
        "    print(\"Removing extra whitespaces from relevant columns...\")\n",
        "\n",
        "    for col in cols_to_clean_whitespace:\n",
        "        if col in df.columns:\n",
        "            # Check if the column is of object (string) type\n",
        "            if pd.api.types.is_object_dtype(df[col]):\n",
        "                # Ensure all elements are strings\n",
        "                df[col] = df[col].astype(str)\n",
        "\n",
        "                # Remove leading and trailing whitespace\n",
        "                df[col] = df[col].str.strip()\n",
        "                # Replace multiple internal whitespaces with a single space\n",
        "                # Use regex '\\s+' to match one or more whitespace characters\n",
        "                df[col] = df[col].str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "                print(f\"Successfully cleaned whitespaces in '{col}'.\")\n",
        "            else:\n",
        "                 print(f\"Column '{col}' is not of object type, skipping whitespace removal.\")\n",
        "        else:\n",
        "            print(f\"Column '{col}' not found in DataFrame, skipping whitespace removal.\")\n",
        "\n",
        "    print(\"\\nFirst 5 rows of the DataFrame after removing whitespaces:\")\n",
        "    print(df.head())\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping Whitespace Removal: Dataset not loaded.\")"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text (Conceptual Code - Likely Not Needed for your current dataset)\n",
        "\n",
        "# This step is typically NOT needed for structured categorical text like Crime Types or Neighborhoods.\n",
        "# It is used for free-form text descriptions if you need to generate alternative phrasings,\n",
        "# which is a more advanced NLP task.\n",
        "\n",
        "# If you had a column like 'Description' that contained narratives,\n",
        "# and needed to rephrase them, you might use libraries like spaCy or NLTK\n",
        "# for simpler rephrasing (e.g., changing sentence structure) or more\n",
        "# advanced models (like T5 or BART from Hugging Face) for complex paraphrasing.\n",
        "\n",
        "# Example using a simple rule-based approach (very limited):\n",
        "# import spacy # You would need to install and load a spaCy model: !pip install spacy; !python -m spacy download en_core_web_sm\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# def simple_rephrase(text):\n",
        "#     if isinstance(text, str):\n",
        "#         doc = nlp(text)\n",
        "#         # A very basic example: reverse subject-verb order if a simple structure is detected\n",
        "#         # This is highly simplified and won't work for most sentences\n",
        "#         rephrased_sentences = []\n",
        "#         for sent in doc.sents:\n",
        "#             # Complex logic involving dependency parsing to find subject and verb...\n",
        "#             rephrased_sentences.append(str(sent)) # Placeholder\n",
        "#         return \" \".join(rephrased_sentences)\n",
        "#     else:\n",
        "#         return text\n",
        "\n",
        "# if df is not None and 'Description' in df.columns: # Replace 'Description' with your actual text column name\n",
        "#     print(\"Attempting to rephrase 'Description' column (using a placeholder/simple method)...\")\n",
        "#     # Apply the rephrasing function\n",
        "#     # df['Description_rephrased'] = df['Description'].apply(simple_rephrase)\n",
        "#     print(\"Rephrasing complete (Note: This requires a proper NLP model for meaningful results).\")\n",
        "# else:\n",
        "#     print(\"\\nSkipping Rephrase Text: Dataset not loaded or no relevant text column found for rephrasing.\")\n",
        "\n",
        "print(\"Rephrasing text is likely not a necessary data cleaning step for the current columns (TYPE, HUNDRED_BLOCK, NEIGHBOURHOOD) in your dataset.\")\n",
        "print(\"This step is typically used for free-form textual descriptions if needed for specific advanced NLP tasks.\")"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re # Often useful for additional cleaning before tokenization\n",
        "\n",
        "# --- Download NLTK punkt tokenizer if not already downloaded ---\n",
        "try:\n",
        "    # Try accessing the standard punkt tokenizer\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    print(\"NLTK punkt tokenizer not found. Downloading...\")\n",
        "    nltk.download('punkt')\n",
        "    print(\"NLTK punkt tokenizer downloaded.\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# --- Download NLTK punkt_tab tokenizer if not already downloaded ---\n",
        "# The traceback specifically requested punkt_tab, which might be a separate or related resource\n",
        "try:\n",
        "    # Try accessing the punkt_tab resource\n",
        "    # The specific path requested in the traceback is 'tokenizers/punkt_tab/english/'\n",
        "    # We can try finding the base 'punkt_tab' resource\n",
        "    nltk.data.find('tokenizers/punkt_tab') # Try to find the base directory\n",
        "except LookupError:\n",
        "    print(\"NLTK punkt_tab resource not found. Downloading...\")\n",
        "    # Download 'punkt_tab' as suggested by the traceback\n",
        "    nltk.download('punkt_tab')\n",
        "    print(\"NLTK punkt_tab resource downloaded.\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# Check if the DataFrame is loaded\n",
        "if df is not None:\n",
        "\n",
        "    # List of columns to tokenize\n",
        "    # These are typically the text/categorical columns\n",
        "    cols_to_tokenize = ['TYPE', 'HUNDRED_BLOCK', 'NEIGHBOURHOOD'] # Add other text columns if applicable\n",
        "\n",
        "    print(\"Performing tokenization on relevant columns...\")\n",
        "\n",
        "    # Function to tokenize text\n",
        "    def tokenize_text(text):\n",
        "        if isinstance(text, str):\n",
        "            # Basic cleaning before tokenization (optional, but recommended after previous steps)\n",
        "            # Ensure lowercase (if not already done) and remove leading/trailing/extra spaces\n",
        "            text = text.lower().strip() # Assumes lowercasing/whitespace cleaning done\n",
        "            text = re.sub(r'\\s+', ' ', text) # Replace multiple spaces with single space\n",
        "\n",
        "            # Use NLTK's word_tokenize\n",
        "            # word_tokenize relies on sent_tokenize, which uses the punkt resource\n",
        "            # Downloading both 'punkt' and 'punkt_tab' should resolve the LookupError\n",
        "            tokens = word_tokenize(text)\n",
        "            return tokens\n",
        "        else:\n",
        "            return [] # Return empty list for non-string data\n",
        "\n",
        "    # Create new columns to store the tokens (e.g., 'TYPE_tokens')\n",
        "    for col in cols_to_tokenize:\n",
        "        if col in df.columns:\n",
        "            # Check if the column is of object (string) type\n",
        "            if pd.api.types.is_object_dtype(df[col]):\n",
        "                new_col_name = f\"{col}_tokens\"\n",
        "                df[new_col_name] = df[col].apply(tokenize_text)\n",
        "                print(f\"Successfully tokenized '{col}' into '{new_col_name}'.\")\n",
        "            else:\n",
        "                 print(f\"Column '{col}' is not of object type, skipping tokenization.\")\n",
        "        else:\n",
        "            print(f\"Column '{col}' not found in DataFrame, skipping tokenization.\")\n",
        "\n",
        "\n",
        "    print(\"\\nFirst 5 rows of the DataFrame after tokenization (showing new token columns):\")\n",
        "    # Display original and new token columns\n",
        "    cols_to_display = [col for col in cols_to_tokenize if f\"{col}_tokens\" in df.columns]\n",
        "    original_and_token_cols = []\n",
        "    for col in cols_to_display:\n",
        "        original_and_token_cols.extend([col, f\"{col}_tokens\"])\n",
        "\n",
        "    if original_and_token_cols:\n",
        "        print(df[original_and_token_cols].head())\n",
        "    else:\n",
        "        print(\"No tokenized columns were created.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping Tokenization: Dataset not loaded.\")"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer # For stemming\n",
        "from nltk.stem import WordNetLemmatizer # For lemmatization\n",
        "# Need the WordNet corpus for lemmatization\n",
        "# Need the OMW (Open Multilingual Wordnet) for some NLTK operations, often paired with WordNet\n",
        "\n",
        "# --- Download NLTK resources for Normalization if not already downloaded ---\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    print(\"NLTK WordNet corpus not found. Downloading...\")\n",
        "    nltk.download('wordnet')\n",
        "    print(\"NLTK WordNet corpus downloaded.\")\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/omw-1.4')\n",
        "except LookupError:\n",
        "    print(\"NLTK OMW 1.4 corpus not found. Downloading...\")\n",
        "    nltk.download('omw-1.4')\n",
        "    print(\"NLTK OMW 1.4 corpus downloaded.\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Check if the DataFrame is loaded and has tokenized columns\n",
        "# Assuming the previous tokenization step was successful and created columns like 'TYPE_tokens'\n",
        "cols_to_normalize = ['TYPE', 'HUNDRED_BLOCK', 'NEIGHBOURHOOD'] # Original columns that were tokenized\n",
        "token_cols = [f\"{col}_tokens\" for col in cols_to_normalize if f\"{col}_tokens\" in df.columns]\n",
        "\n",
        "if df is not None and token_cols:\n",
        "\n",
        "    print(\"Performing text normalization (Stemming and Lemmatization) on tokenized columns...\")\n",
        "\n",
        "    # Initialize Stemmer and Lemmatizer\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Function to apply stemming to a list of tokens\n",
        "    def apply_stemming(tokens):\n",
        "        if isinstance(tokens, list):\n",
        "            return [stemmer.stem(token) for token in tokens]\n",
        "        else:\n",
        "            return [] # Return empty list for non-list data\n",
        "\n",
        "    # Function to apply lemmatization to a list of tokens\n",
        "    # Lemmatization often requires a Part-of-Speech (POS) tag, but for simplicity,\n",
        "    # we'll use the default 'n' (noun) tag. For more accurate lemmatization,\n",
        "    # you would need to perform POS tagging first.\n",
        "    def apply_lemmatization(tokens):\n",
        "         if isinstance(tokens, list):\n",
        "            # We'll default to 'n' (noun) POS tag for simplicity\n",
        "            return [lemmatizer.lemmatize(token, pos='n') for token in tokens] # Default pos='n'\n",
        "         else:\n",
        "            return [] # Return empty list for non-list data\n",
        "\n",
        "\n",
        "    # Apply stemming and lemmatization to the tokenized columns\n",
        "    for col_tokens in token_cols:\n",
        "        original_col_name = col_tokens.replace('_tokens', '') # Get original column name\n",
        "\n",
        "        # Apply Stemming\n",
        "        new_stem_col_name = f\"{original_col_name}_stemmed\"\n",
        "        df[new_stem_col_name] = df[col_tokens].apply(apply_stemming)\n",
        "        print(f\"Successfully stemmed tokens in '{col_tokens}' into '{new_stem_col_name}'.\")\n",
        "\n",
        "        # Apply Lemmatization\n",
        "        new_lemma_col_name = f\"{original_col_name}_lemmatized\"\n",
        "        df[new_lemma_col_name] = df[col_tokens].apply(apply_lemmatization)\n",
        "        print(f\"Successfully lemmatized tokens in '{col_tokens}' into '{new_lemma_col_name}'.\")\n",
        "\n",
        "\n",
        "    print(\"\\nFirst 5 rows of the DataFrame after normalization (showing new stemmed and lemmatized columns):\")\n",
        "    # Display original tokenized, stemmed, and lemmatized columns\n",
        "    cols_to_display = []\n",
        "    for col in cols_to_normalize:\n",
        "         if f\"{col}_tokens\" in df.columns:\n",
        "             cols_to_display.append(f\"{col}_tokens\")\n",
        "         if f\"{col}_stemmed\" in df.columns:\n",
        "             cols_to_display.append(f\"{col}_stemmed\")\n",
        "         if f\"{col}_lemmatized\" in df.columns:\n",
        "             cols_to_display.append(f\"{col}_lemmatized\")\n",
        "\n",
        "    if cols_to_display:\n",
        "        print(df[cols_to_display].head())\n",
        "    else:\n",
        "        print(\"No normalized columns were created. Ensure tokenization step was successful.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping Text Normalization: Dataset not loaded or tokenized columns not found.\")"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Tagging\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "# No need to import stemmer/lemmatizer again if already imported in the previous cell\n",
        "\n",
        "# --- Download NLTK POS tagger (specifically the English version) if not already downloaded ---\n",
        "try:\n",
        "    # Explicitly check for the language-specific resource\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n",
        "except LookupError:\n",
        "    print(\"NLTK averaged_perceptron_tagger_eng not found. Downloading...\")\n",
        "    # Download the specific English tagger\n",
        "    nltk.download('averaged_perceptron_tagger_eng')\n",
        "    print(\"NLTK averaged_perceptron_tagger_eng downloaded.\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Check if the DataFrame is loaded and has tokenized columns\n",
        "# Assuming the previous tokenization step was successful and created columns like 'TYPE_tokens'\n",
        "cols_to_tag = ['TYPE', 'HUNDRED_BLOCK', 'NEIGHBOURHOOD'] # Original columns that were tokenized\n",
        "token_cols = [f\"{col}_tokens\" for col in cols_to_tag if f\"{col}_tokens\" in df.columns]\n",
        "\n",
        "\n",
        "if df is not None and token_cols:\n",
        "\n",
        "    print(\"Performing Part-of-Speech (POS) tagging on tokenized columns...\")\n",
        "\n",
        "    # Function to apply POS tagging to a list of tokens\n",
        "    def apply_pos_tagging(tokens):\n",
        "        if isinstance(tokens, list):\n",
        "            # nltk.pos_tag takes a list of tokens\n",
        "            return nltk.pos_tag(tokens)\n",
        "        else:\n",
        "            return [] # Return empty list for non-list data\n",
        "\n",
        "\n",
        "    # Apply POS tagging to the tokenized columns\n",
        "    for col_tokens in token_cols:\n",
        "        original_col_name = col_tokens.replace('_tokens', '') # Get original column name\n",
        "\n",
        "        # Apply POS Tagging\n",
        "        new_pos_col_name = f\"{original_col_name}_pos_tags\"\n",
        "        df[new_pos_col_name] = df[col_tokens].apply(apply_pos_tagging)\n",
        "        print(f\"Successfully applied POS tagging to tokens in '{col_tokens}' into '{new_pos_col_name}'.\")\n",
        "\n",
        "\n",
        "    print(\"\\nFirst 5 rows of the DataFrame after POS tagging (showing new POS tag columns):\")\n",
        "    # Display original tokenized and new POS tag columns\n",
        "    cols_to_display = []\n",
        "    for col in cols_to_tag:\n",
        "         if f\"{col}_tokens\" in df.columns:\n",
        "             cols_to_display.append(f\"{col}_tokens\")\n",
        "         if f\"{col}_pos_tags\" in df.columns:\n",
        "             cols_to_display.append(f\"{col}_pos_tags\")\n",
        "\n",
        "    if cols_to_display:\n",
        "        print(df[cols_to_display].head())\n",
        "    else:\n",
        "        print(\"No POS tagged columns were created. Ensure tokenization step was successful.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping POS Tagging: Dataset not loaded or tokenized columns not found.\")"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text vectorization\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Check if the DataFrame is loaded and has the necessary columns for vectorization\n",
        "# We will vectorize the stemmed or lemmatized columns as they are normalized\n",
        "# Prioritize lemmatized if available, otherwise use stemmed\n",
        "cols_to_vectorize = ['TYPE', 'HUNDRED_BLOCK', 'NEIGHBOURHOOD']\n",
        "normalized_cols = []\n",
        "\n",
        "for col in cols_to_vectorize:\n",
        "    lemmatized_col = f\"{col}_lemmatized\"\n",
        "    stemmed_col = f\"{col}_stemmed\"\n",
        "    if lemmatized_col in df.columns:\n",
        "        normalized_cols.append(lemmatized_col)\n",
        "    elif stemmed_col in df.columns:\n",
        "        normalized_cols.append(stemmed_col)\n",
        "    else:\n",
        "        print(f\"Warning: Neither '{lemmatized_col}' nor '{stemmed_col}' found for column '{col}'. Skipping.\")\n",
        "\n",
        "\n",
        "if df is not None and normalized_cols:\n",
        "\n",
        "    print(\"Performing TF-IDF vectorization on normalized text columns...\")\n",
        "\n",
        "    # Initialize a dictionary to store TF-IDF matrices for each column\n",
        "    tfidf_matrices = {}\n",
        "\n",
        "    for col in normalized_cols:\n",
        "        print(f\"Vectorizing column: '{col}'\")\n",
        "        # TF-IDF Vectorizer\n",
        "        # Convert list of tokens back to strings for TF-IDF\n",
        "        # Handle potential non-list entries gracefully\n",
        "        df[col] = df[col].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')\n",
        "\n",
        "        tfidf_vectorizer = TfidfVectorizer(max_features=1000) # Limit features to 1000 for manageability\n",
        "        try:\n",
        "            # Fit and transform the text data\n",
        "            tfidf_matrix = tfidf_vectorizer.fit_transform(df[col])\n",
        "\n",
        "            # Store the matrix\n",
        "            tfidf_matrices[col] = tfidf_matrix\n",
        "            print(f\"Successfully created TF-IDF matrix for '{col}'. Shape: {tfidf_matrix.shape}\")\n",
        "\n",
        "            # Optionally, display the feature names (words) learned by the vectorizer\n",
        "            # print(f\"Features for '{col}': {tfidf_vectorizer.get_feature_names_out()[:50]}...\") # Display first 50 features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during TF-IDF vectorization for column '{col}': {e}\")\n",
        "\n",
        "\n",
        "    print(\"\\nTF-IDF Vectorization Complete.\")\n",
        "    print(\"TF-IDF matrices are stored in the 'tfidf_matrices' dictionary.\")\n",
        "\n",
        "    # Example of how to access a matrix (e.g., for 'TYPE_lemmatized')\n",
        "    # if 'TYPE_lemmatized' in tfidf_matrices:\n",
        "    #     print(\"\\nExample access: TF-IDF matrix for 'TYPE_lemmatized':\")\n",
        "    #     print(tfidf_matrices['TYPE_lemmatized'][:5].toarray()) # Display first 5 rows as dense array\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping TF-IDF Vectorization: Dataset not loaded or necessary normalized columns not found.\")"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Assuming high correlation between X/Y and Longitude/Latitude\n",
        "# Decide which set to keep based on your analysis and ease of use (Latitude/Longitude are generally preferred for mapping)\n",
        "# If X and Longitude are highly correlated, and Y and Latitude are highly correlated:\n",
        "# Drop X and Y\n",
        "if df is not None:\n",
        "    if 'X' in df.columns and 'Y' in df.columns:\n",
        "        df = df.drop(['X', 'Y'], axis=1)\n",
        "        print(\"Dropped 'X' and 'Y' columns due to high correlation with Latitude and Longitude.\")\n",
        "    else:\n",
        "        print(\"'X' or 'Y' columns not found. Skipping dropping.\")\n",
        "else:\n",
        "    print(\"DataFrame not loaded. Cannot drop columns.\")\n",
        "\n",
        "# You might also check for correlations between other numerical features and decide based on your model's requirements.\n",
        "# For example, if YEAR and MONTH are highly correlated with a newly created 'Timestamp' feature,\n",
        "# you might consider dropping YEAR and MONTH if the timestamp is sufficient."
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "import pandas as pd\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Assume 'df' is your DataFrame after data cleaning and initial feature engineering\n",
        "# Make sure you have loaded and processed your data before running this code.\n",
        "# If df is None from previous steps, load it again or ensure the previous steps ran successfully.\n",
        "if 'df' not in locals() or df is None:\n",
        "    print(\"DataFrame 'df' not found or is None. Please ensure your data loading and initial processing steps have run.\")\n",
        "    # If you need to reload the data for this section, uncomment and modify the following:\n",
        "    # try:\n",
        "    #     df = pd.read_excel('/content/drive/MyDrive/Train.xlsx')\n",
        "    # except FileNotFoundError:\n",
        "    #     print(\"Error: The file 'Train.xlsx' was not found.\")\n",
        "    #     # Exit or handle the error appropriately\n",
        "    #     # exit() # Example: exit the cell execution if in a script\n",
        "    #     pass # Allow the script to continue, but df will be None\n",
        "\n",
        "    # # Assuming you also added some basic time features as done previously\n",
        "    # if df is not None and 'Date' in df.columns:\n",
        "    #      try:\n",
        "    #          df['Date'] = pd.to_datetime(df['Date'])\n",
        "    #          df['Day_of_Week'] = df['Date'].dt.dayofweek\n",
        "    #          df['Month'] = df['Date'].dt.month\n",
        "    #          df['Year'] = df['Date'].dt.year\n",
        "    #          df['HOUR'] = df['Date'].dt.hour\n",
        "    #          df['MINUTE'] = df['Date'].dt.minute\n",
        "    #          df['Time_of_Day'] = df['HOUR'] + df['MINUTE']/60\n",
        "    #          # Assuming Is_Weekend and Quarter were created elsewhere or need creation here\n",
        "    #          # Example:\n",
        "    #          # df['Is_Weekend'] = df['Date'].dt.dayofweek >= 5\n",
        "    #          # df['Quarter'] = df['Date'].dt.quarter\n",
        "    #      except Exception as e:\n",
        "    #          print(f\"Error processing 'Date' column during reload: {e}\")\n",
        "\n",
        "\n",
        "# --- Step 1: Check for Multicollinearity using VIF ---\n",
        "\n",
        "if df is not None:\n",
        "    print(\"--- Checking for Multicollinearity (VIF) ---\")\n",
        "\n",
        "    # Select only numerical columns for VIF calculation\n",
        "    # Exclude the target variable if you have one and it's numerical\n",
        "    # Make sure to exclude any identifier columns that aren't features\n",
        "    numerical_features = df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "    # Remove Latitude and Longitude if they were kept and you suspect high correlation\n",
        "    # This is based on the assumption from your previous step. Adjust if needed.\n",
        "    features_to_exclude_from_vif = []\n",
        "    if 'Latitude' in numerical_features and 'Longitude' in numerical_features:\n",
        "         # Assuming you kept Lat/Lon and dropped X/Y\n",
        "         # If you have other highly correlated features identified from the heatmap, add them here\n",
        "         pass # Keep Lat/Lon for VIF calculation unless you have reason to drop them before this step\n",
        "\n",
        "    # Exclude the original X and Y if they still exist (they should have been dropped earlier)\n",
        "    if 'X' in numerical_features:\n",
        "        features_to_exclude_from_vif.append('X')\n",
        "    if 'Y' in numerical_features:\n",
        "        features_to_exclude_from_vif.append('Y')\n",
        "    if 'MINUTE' in numerical_features:\n",
        "         # Minute might have low variance or be highly correlated with Time_of_Day\n",
        "         # You can decide to exclude it from VIF or drop it as a feature later\n",
        "         pass # Keep Minute for now to see its VIF\n",
        "\n",
        "    numerical_features_for_vif = [f for f in numerical_features if f not in features_to_exclude_from_vif]\n",
        "\n",
        "    # Drop rows with NaN values in the selected numerical columns for VIF calculation\n",
        "    # VIF cannot handle NaN values. Be mindful of how you handle NaNs in your overall workflow.\n",
        "    df_for_vif = df[numerical_features_for_vif].dropna()\n",
        "\n",
        "    if not df_for_vif.empty:\n",
        "        # Calculate VIF\n",
        "        vif_data = pd.DataFrame()\n",
        "        vif_data[\"feature\"] = df_for_vif.columns\n",
        "        # Ensure the input to variance_inflation_factor is a numpy array of floats\n",
        "        vif_data[\"VIF\"] = [variance_inflation_factor(df_for_vif.values.astype(float), i) for i in range(len(df_for_vif.columns))]\n",
        "\n",
        "        print(\"\\nVariance Inflation Factor (VIF) for Numerical Features:\")\n",
        "        print(vif_data.sort_values(by='VIF', ascending=False))\n",
        "\n",
        "        # Identify features with high VIF (e.g., VIF > 5 or 10)\n",
        "        high_vif_features = vif_data[vif_data['VIF'] > 5]['feature'].tolist() # Using 5 as a common threshold\n",
        "        print(f\"\\nFeatures with VIF > 5: {high_vif_features}\")\n",
        "\n",
        "        # Decision based on VIF: Consider removing features with very high VIF,\n",
        "        # especially if they are highly correlated with other features you intend to keep.\n",
        "        # For example, if 'Time_of_Day' and 'HOUR' both have high VIF and are highly correlated,\n",
        "        # you might choose to keep only 'Time_of_Day'.\n",
        "\n",
        "    else:\n",
        "        print(\"DataFrame for VIF calculation is empty after dropping NaNs. Cannot calculate VIF.\")\n",
        "\n",
        "    # --- Step 2: Consider Categorical Features ---\n",
        "\n",
        "    print(\"\\n--- Considering Categorical Features ---\")\n",
        "\n",
        "    # Analyze the cardinality of categorical features\n",
        "    # Get object type columns and ensure they are not datetime or other complex types masquerading as object\n",
        "    categorical_features = df.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "    print(\"\\nCardinality of Categorical Features:\")\n",
        "    for col in categorical_features:\n",
        "        try:\n",
        "            # Check if the column contains lists or other unhashable types\n",
        "            # Sample the first few non-null values to get an idea of the type\n",
        "            sample_values = df[col].dropna().head()\n",
        "            contains_lists = any(isinstance(x, list) for x in sample_values)\n",
        "\n",
        "            if contains_lists:\n",
        "                print(f\"{col}: Contains lists or unhashable types. Cannot calculate simple nunique().\")\n",
        "                # If you need to count unique lists (treating each list as a single item),\n",
        "                # you could convert the lists to hashable types like tuples:\n",
        "                # print(f\"{col} (treating lists as unique): {df[col].apply(lambda x: tuple(x) if isinstance(x, list) else x).nunique()} unique values\")\n",
        "                # Or convert to string representation if the list content is what matters\n",
        "                # print(f\"{col} (as string): {df[col].astype(str).nunique()} unique values\")\n",
        "            else:\n",
        "                print(f\"{col}: {df[col].nunique()} unique values\")\n",
        "        except TypeError as e:\n",
        "            print(f\"Error calculating nunique for column '{col}': {e}\")\n",
        "            print(f\"Check the data type of values in column '{col}'. It likely contains unhashable types like lists.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred for column '{col}': {e}\")\n",
        "\n",
        "\n",
        "    # Based on cardinality and relevance, decide how to handle them for modeling:\n",
        "    # - Low cardinality (e.g., 'Day_of_Week', 'Is_Weekend'): One-Hot Encoding is often suitable.\n",
        "    # - High cardinality (e.g., 'NEIGHBOURHOOD', 'HUNDRED_BLOCK'):\n",
        "    #   - Group rare categories.\n",
        "    #   - Use more advanced encoding techniques (Target Encoding, etc.).\n",
        "    #   - Consider if the feature is truly necessary for your forecasting task at the level of granularity provided.\n",
        "\n",
        "    # --- Step 3: Feature Selection Strategy Based on Exploration and Domain Knowledge ---\n",
        "\n",
        "    print(\"\\n--- Feature Selection Strategy ---\")\n",
        "\n",
        "    # Based on your EDA and the VIF analysis, list the features you plan to use for your model.\n",
        "    # This is a conceptual step based on your analysis. You'll implement the actual\n",
        "    # dropping/keeping of columns before model training.\n",
        "\n",
        "    # Example: Let's assume based on your analysis, you decide to keep the following:\n",
        "    # - Time-based features: Date (as index), Day_of_Week, Month, Year, Time_of_Day, Is_Weekend, Quarter\n",
        "    # - Location-based features: Latitude, Longitude (assuming they are sufficiently informative and not perfectly collinear after dropping X/Y)\n",
        "    # - Crime Type (as you are forecasting crime trends)\n",
        "    # - Neighborhood (handled appropriately based on cardinality)\n",
        "\n",
        "    selected_features_for_modeling = [\n",
        "        # Assuming these columns exist after previous steps and handling of NaNs/types\n",
        "        'Date', # Often used as the index for time series\n",
        "        'Day_of_Week',\n",
        "        'Month',\n",
        "        'Year',\n",
        "        'Time_of_Day',\n",
        "        # 'Is_Weekend', # Ensure this was created\n",
        "        # 'Quarter',    # Ensure this was created\n",
        "        'Latitude',\n",
        "        'Longitude',\n",
        "        'TYPE', # Or encode TYPE if it's your target or a predictor\n",
        "        'NEIGHBOURHOOD' # Will require appropriate encoding\n",
        "        # Add any other relevant features you engineered or decided to keep\n",
        "    ]\n",
        "\n",
        "    print(\"\\nPotential features for modeling based on analysis:\")\n",
        "    # Filter the list to only include columns that actually exist in the DataFrame\n",
        "    actual_selected_features = [f for f in selected_features_for_modeling if f in df.columns]\n",
        "    print(actual_selected_features)\n",
        "\n",
        "    # Note: You will need to handle the categorical features ('TYPE', 'NEIGHBOURHOOD')\n",
        "    # before feeding them into your time series forecasting models.\n",
        "    # The encoding strategy will depend on the specific model you choose.\n",
        "\n",
        "    print(\"\\nNext Steps:\")\n",
        "    print(\"1. Decide on the final set of features based on VIF, cardinality, and relevance.\")\n",
        "    print(\"2. Implement appropriate encoding for categorical features.\")\n",
        "    print(\"3. Prepare your data for the chosen time series model (e.g., creating time series index, handling NaNs).\")\n",
        "    print(\"4. Split data into training and validation/test sets using time-based splitting.\")\n",
        "    print(\"5. Train your model and evaluate performance using time series cross-validation.\")\n",
        "    print(\"6. Iterate on feature selection and model parameters based on performance.\")\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame not loaded. Cannot perform feature selection steps.\")"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# This section is for applying transformations like encoding categorical features,\n",
        "# scaling numerical features, handling outliers if not done before, and preparing\n",
        "# the data specifically for your chosen time series forecasting model.\n",
        "\n",
        "# Based on the previous analysis, let's assume the following:\n",
        "# - 'Date' is used as the time index.\n",
        "# - 'TYPE' and 'NEIGHBOURHOOD' are categorical features that need encoding.\n",
        "# - Numerical features ('Latitude', 'Longitude', 'HOUR', 'MINUTE', 'YEAR', 'MONTH', 'DAY', 'Time_of_Day', 'Day_of_Week') might benefit from scaling depending on the model.\n",
        "# - You need to decide on a target variable for forecasting (e.g., count of incidents per time period/location, or forecasting a specific crime type).\n",
        "\n",
        "# --- Example Transformations ---\n",
        "\n",
        "if df is not None:\n",
        "    print(\"--- Applying Data Transformations ---\")\n",
        "\n",
        "    # 1. Handling Categorical Features (TYPE and NEIGHBOURHOOD)\n",
        "    # The best encoding strategy depends on your model.\n",
        "    # One-Hot Encoding is simple but can create many columns for high cardinality features.\n",
        "    # Target Encoding or grouping rare categories might be better for high cardinality.\n",
        "\n",
        "    # Let's demonstrate One-Hot Encoding for 'TYPE' (assuming manageable number of types)\n",
        "    # and a strategy for 'NEIGHBOURHOOD' (e.g., keeping top N and grouping the rest)\n",
        "    # Decide on the number of top neighborhoods to keep\n",
        "    top_n_neighborhoods_to_encode = 50 # Adjust based on your data and analysis\n",
        "\n",
        "    if 'TYPE' in df.columns:\n",
        "        print(\"\\nApplying One-Hot Encoding for 'TYPE'...\")\n",
        "        # Use get_dummies for one-hot encoding\n",
        "        # prefix='TYPE' adds a prefix to the new columns for clarity\n",
        "        # dummy_na=False excludes NaN values (you should handle NaNs before this)\n",
        "        df = pd.get_dummies(df, columns=['TYPE'], prefix='TYPE', dummy_na=False)\n",
        "        print(\"One-Hot Encoding for 'TYPE' applied.\")\n",
        "    else:\n",
        "        print(\"'TYPE' column not found. Skipping One-Hot Encoding for TYPE.\")\n",
        "\n",
        "\n",
        "    if 'NEIGHBOURHOOD' in df.columns:\n",
        "        print(f\"\\nHandling 'NEIGHBOURHOOD' by keeping top {top_n_neighborhoods_to_encode} and grouping others...\")\n",
        "        # Get the value counts and identify the top neighborhoods\n",
        "        neighborhood_counts = df['NEIGHBOURHOOD'].value_counts()\n",
        "        if len(neighborhood_counts) > top_n_neighborhoods_to_encode:\n",
        "            top_neighborhoods_list = neighborhood_counts.nlargest(top_n_neighborhoods_to_encode).index.tolist()\n",
        "            # Replace neighborhoods not in the top list with 'Other'\n",
        "            df['NEIGHBOURHOOD_Grouped'] = df['NEIGHBOURHOOD'].apply(lambda x: x if x in top_neighborhoods_list else 'Other')\n",
        "\n",
        "            # Now, One-Hot Encode the grouped neighborhood column\n",
        "            print(\"Applying One-Hot Encoding for 'NEIGHBOURHOOD_Grouped'...\")\n",
        "            df = pd.get_dummies(df, columns=['NEIGHBOURHOOD_Grouped'], prefix='NEIGHBOURHOOD', dummy_na=False)\n",
        "            print(\"One-Hot Encoding for grouped neighborhoods applied.\")\n",
        "            # You might choose to drop the original 'NEIGHBOURHOOD' column if you no longer need it\n",
        "            df = df.drop('NEIGHBOURHOOD', axis=1)\n",
        "        else:\n",
        "            # If there are fewer neighborhoods than the threshold, just encode all of them\n",
        "            print(\"Number of neighborhoods is less than the threshold. Applying One-Hot Encoding to all neighborhoods.\")\n",
        "            df = pd.get_dummies(df, columns=['NEIGHBOURHOOD'], prefix='NEIGHBOURHOOD', dummy_na=False)\n",
        "            print(\"One-Hot Encoding for 'NEIGHBOURHOOD' applied.\")\n",
        "    else:\n",
        "        print(\"'NEIGHBOURHOOD' column not found. Skipping neighborhood encoding.\")\n",
        "\n",
        "\n",
        "    # 2. Feature Scaling (for numerical features)\n",
        "    # Scaling is often important for distance-based algorithms or models sensitive to feature scales (like LSTMs, SVMs, etc.)\n",
        "    # Tree-based models (like Random Forest, Gradient Boosting) are generally not sensitive to scaling.\n",
        "    # Decide which numerical features need scaling based on your model choice.\n",
        "\n",
        "    # Example: Scaling Latitude and Longitude if they are features\n",
        "    numerical_cols_to_scale = ['Latitude', 'Longitude', 'HOUR', 'MINUTE', 'YEAR', 'MONTH', 'DAY']\n",
        "    # Check if these columns exist before attempting to scale\n",
        "    numerical_cols_to_scale_exist = [col for col in numerical_cols_to_scale if col in df.columns]\n",
        "\n",
        "    if numerical_cols_to_scale_exist:\n",
        "        print(\"\\nApplying StandardScaler to selected numerical features...\")\n",
        "        from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "        # Create a scaler object\n",
        "        scaler = StandardScaler()\n",
        "\n",
        "        # Fit the scaler to the selected columns and transform the data\n",
        "        # Ensure no infinite values exist before scaling\n",
        "        df[numerical_cols_to_scale_exist] = df[numerical_cols_to_scale_exist].replace([np.inf, -np.inf], np.nan)\n",
        "        # Handle NaNs before scaling (e.g., imputation or dropping)\n",
        "        # For demonstration, let's fill NaNs with the mean (choose an appropriate strategy for your data)\n",
        "        df[numerical_cols_to_scale_exist] = df[numerical_cols_to_scale_exist].fillna(df[numerical_cols_to_scale_exist].mean())\n",
        "\n",
        "\n",
        "        df[numerical_cols_to_scale_exist] = scaler.fit_transform(df[numerical_cols_to_scale_exist])\n",
        "        print(\"StandardScaler applied.\")\n",
        "        print(f\"Scaled columns: {numerical_cols_to_scale_exist}\")\n",
        "    else:\n",
        "         print(\"\\nNo numerical columns found for scaling from the specified list.\")\n",
        "\n",
        "\n",
        "    # 3. Creating a Time Series Index\n",
        "    # For time series forecasting, it's standard to have a datetime index.\n",
        "    # Ensure your 'Date' column is in datetime format.\n",
        "    if 'Date' in df.columns:\n",
        "         try:\n",
        "             print(\"\\nSetting 'Date' as DataFrame index...\")\n",
        "             # Convert 'Date' to datetime if not already\n",
        "             df['Date'] = pd.to_datetime(df['Date'])\n",
        "             # Set 'Date' as index and sort by index\n",
        "             df = df.set_index('Date').sort_index()\n",
        "             print(\"'Date' column set as index.\")\n",
        "         except Exception as e:\n",
        "             print(f\"Error setting 'Date' as index: {e}\")\n",
        "             print(\"Please ensure the 'Date' column is available and correctly formatted.\")\n",
        "    else:\n",
        "        print(\"\\n'Date' column not found. Cannot set time series index.\")\n",
        "        print(\"Please ensure the 'Date' column was created/loaded and is in datetime format.\")\n",
        "\n",
        "    # 4. Define Target Variable\n",
        "    # What are you forecasting? Total crime count? Count of a specific crime type?\n",
        "    # This step is conceptual as the target depends on your specific forecasting problem.\n",
        "    # Example: If forecasting total daily crime count:\n",
        "    # You would first need to aggregate the data by day and count incidents.\n",
        "    # target = df.resample('D').size().rename('daily_crime_count')\n",
        "    # Then, merge this target with your features, ensuring alignment by date.\n",
        "\n",
        "\n",
        "    print(\"\\n--- Transformations Complete ---\")\n",
        "    print(\"DataFrame head after transformations:\")\n",
        "    display(df.head())\n",
        "    print(\"\\nDataFrame info after transformations:\")\n",
        "    df.info()\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame not loaded. Cannot apply transformations.\")"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "\n",
        "# This section specifically focuses on scaling numerical features.\n",
        "# Scaling is important for many machine learning algorithms, particularly those\n",
        "# that are sensitive to the magnitude of features, such as:\n",
        "# - Gradient Descent based optimizers (used in Neural Networks, Logistic Regression, SVMs)\n",
        "# - Distance-based algorithms (K-Nearest Neighbors, K-Means Clustering)\n",
        "# - Principal Component Analysis (PCA)\n",
        "\n",
        "# Tree-based models like Decision Trees, Random Forests, and Gradient Boosting\n",
        "# (e.g., LightGBM, XGBoost) are generally not affected by feature scaling.\n",
        "\n",
        "# Choose the appropriate scaler based on your data distribution and model requirements.\n",
        "# Common scalers include:\n",
        "# - StandardScaler: Standardizes features by removing the mean and scaling to unit variance (mean=0, variance=1). Suitable for data that is approximately normally distributed.\n",
        "# - MinMaxScaler: Scales features to a fixed range, usually [0, 1] or [-1, 1]. Useful when you need to maintain the original distribution shape or when the data is not Gaussian.\n",
        "# - RobustScaler: Scales features using statistics that are robust to outliers (median and interquartile range). Good if your data contains many outliers.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "\n",
        "if df is not None:\n",
        "    print(\"--- Applying Feature Scaling ---\")\n",
        "\n",
        "    # Identify the numerical columns to scale\n",
        "    # Exclude columns that are identifiers, target variables (if already defined and you don't want to scale it),\n",
        "    # or already encoded categorical features.\n",
        "    # Use select_dtypes to get current numerical columns.\n",
        "    numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "    # Based on your previous steps, some columns might already be numerical (e.g., Latitude, Longitude, time components).\n",
        "    # Exclude any columns that resulted from one-hot encoding, as these are already binary (0 or 1).\n",
        "    # Also, exclude any integer columns that represent counts or labels if scaling doesn't make sense for them.\n",
        "    # Let's make a list of columns we *intend* to scale.\n",
        "    # You might need to adjust this list based on your specific DataFrame after transformations.\n",
        "\n",
        "    # Example list of columns to potentially scale:\n",
        "    features_to_scale = ['Latitude', 'Longitude', 'HOUR', 'MINUTE', 'YEAR', 'MONTH', 'DAY', 'Time_of_Day', 'Day_of_Week']\n",
        "\n",
        "    # Filter this list to include only columns that currently exist in the DataFrame and are numerical\n",
        "    # after the previous transformation steps (like one-hot encoding).\n",
        "    # Some columns might have been dropped or created in previous steps.\n",
        "    numerical_features_to_scale_exist = [col for col in features_to_scale if col in df.columns and np.issubdtype(df[col].dtype, np.number)]\n",
        "\n",
        "    if not numerical_features_to_scale_exist:\n",
        "        print(\"No numerical features identified for scaling from the specified list.\")\n",
        "    else:\n",
        "        print(f\"Identified numerical features for scaling: {numerical_features_to_scale_exist}\")\n",
        "\n",
        "        # --- Choose Your Scaler ---\n",
        "        # scaler = StandardScaler() # Uncomment this line to use StandardScaler\n",
        "        scaler = MinMaxScaler() # Uncomment this line to use MinMaxScaler (example)\n",
        "        # scaler = RobustScaler() # Uncomment this line to use RobustScaler\n",
        "\n",
        "        print(f\"Using Scaler: {type(scaler).__name__}\")\n",
        "\n",
        "        try:\n",
        "            # Prepare the data for scaling\n",
        "            # Scaling works best on numerical values. Ensure there are no non-numeric values\n",
        "            # or infinities in the selected columns. Handle NaNs if present.\n",
        "            df_to_scale = df[numerical_features_to_scale_exist].copy()\n",
        "\n",
        "            # Replace infinite values with NaN, then handle NaNs\n",
        "            df_to_scale.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "            # Example NaN handling: Impute with mean or median. Choose a strategy based on your data.\n",
        "            # For StandardScaler/MinMaxScaler, mean/median imputation is common.\n",
        "            # For RobustScaler, median imputation might be more appropriate.\n",
        "            # Let's use the mean for demonstration (adjust as needed)\n",
        "            df_to_scale.fillna(df_to_scale.mean(), inplace=True)\n",
        "            print(f\"Handling NaNs by filling with mean in columns: {numerical_features_to_scale_exist}\")\n",
        "\n",
        "\n",
        "            # Ensure the data is in a suitable format for the scaler (e.g., numpy array)\n",
        "            # The scaler expects a 2D array.\n",
        "            scaled_data = scaler.fit_transform(df_to_scale)\n",
        "\n",
        "            # Create a new DataFrame with the scaled data, keeping the original column names and index\n",
        "            scaled_df = pd.DataFrame(scaled_data, columns=numerical_features_to_scale_exist, index=df.index)\n",
        "\n",
        "            # Replace the original columns in the main DataFrame with the scaled versions\n",
        "            df[numerical_features_to_scale_exist] = scaled_df\n",
        "\n",
        "            print(\"\\nScaling applied successfully.\")\n",
        "            print(\"DataFrame head after scaling selected features:\")\n",
        "            display(df.head())\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during scaling: {e}\")\n",
        "            print(\"Please check the selected columns and their data types.\")\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame not loaded. Cannot perform scaling.\")"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "\n",
        "# Dimensionality reduction is a technique used to reduce the number of features\n",
        "# in a dataset. This can be beneficial for several reasons:\n",
        "# 1. Reduces storage space and computation time.\n",
        "# 2. Helps to mitigate the \"curse of dimensionality\", where data becomes sparse\n",
        "#    in high-dimensional spaces, making it harder for models to find patterns.\n",
        "# 3. Can help remove noise and multicollinearity.\n",
        "# 4. Can aid in visualization by reducing data to 2 or 3 dimensions.\n",
        "\n",
        "# Common dimensionality reduction techniques include:\n",
        "# - Principal Component Analysis (PCA): A linear technique that finds orthogonal\n",
        "#   principal components that capture the maximum variance in the data. Unsupervised.\n",
        "# - t-Distributed Stochastic Neighbor Embedding (t-SNE): A non-linear technique\n",
        "#   primarily used for visualizing high-dimensional data in 2 or 3 dimensions.\n",
        "# - UMAP (Uniform Manifold Approximation and Projection): Another non-linear\n",
        "#   technique for visualization and general purpose dimensionality reduction,\n",
        "#   often faster than t-SNE.\n",
        "# - Factor Analysis, Independent Component Analysis (ICA), Linear Discriminant\n",
        "#   Analysis (LDA - supervised), etc.\n",
        "\n",
        "# Whether dimensionality reduction is \"needed\" depends on:\n",
        "# - The number of features you have after feature engineering and encoding.\n",
        "# - The performance of your chosen time series model with the current number of features.\n",
        "# - Your goal (e.g., visualization, noise reduction, computational efficiency).\n",
        "\n",
        "# For time series forecasting, applying dimensionality reduction directly to the\n",
        "# feature matrix can be tricky if the technique doesn't preserve the temporal\n",
        "# relationships well. PCA, for example, treats all features equally regardless\n",
        "# of their temporal context.\n",
        "\n",
        "# If you have a very large number of static features per time step (e.g., many\n",
        "# one-hot encoded geographical features), PCA might be applied to *those static\n",
        "# features* to reduce their dimension before incorporating them into a time\n",
        "# series model.\n",
        "\n",
        "# Let's illustrate with PCA as it's a common method, but consider if it's truly\n",
        "# appropriate for your time series forecasting context or if other feature\n",
        "# selection methods (based on relevance, domain knowledge, model interpretability)\n",
        "# are more suitable.\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "if df is not None:\n",
        "    print(\"--- Considering Dimensionality Reduction (PCA Example) ---\")\n",
        "\n",
        "    # Identify the numerical features for PCA. This should include scaled numerical\n",
        "    # features and one-hot encoded categorical features.\n",
        "    # Exclude the time index and potentially the target variable if it's in this DataFrame.\n",
        "    # Assuming 'Date' is the index and you don't have a target column yet in this DataFrame.\n",
        "\n",
        "    # Get all numerical columns (including scaled ones and one-hot encoded dummies)\n",
        "    features_for_pca = df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "    # Exclude columns that should not be part of the dimensionality reduction if they exist\n",
        "    # For example, if you have a separate target column named 'crime_count', exclude it.\n",
        "    # target_column = 'your_target_column_name' # Define your target column if applicable\n",
        "    # if target_column in features_for_pca:\n",
        "    #     features_for_pca.remove(target_column)\n",
        "\n",
        "    if not features_for_pca:\n",
        "        print(\"No numerical features available for PCA.\")\n",
        "    else:\n",
        "        print(f\"Features considered for PCA: {features_for_pca}\")\n",
        "\n",
        "        # Prepare data for PCA\n",
        "        # PCA cannot handle NaN values. Ensure NaNs are handled before this step.\n",
        "        # Assuming NaNs were handled during scaling or previous steps.\n",
        "        data_for_pca = df[features_for_pca].copy()\n",
        "\n",
        "        # Optional: Check for and handle any remaining NaNs or infinities\n",
        "        data_for_pca.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "        if data_for_pca.isnull().sum().sum() > 0:\n",
        "             print(\"Warning: NaN values found in data for PCA. Filling with mean for demonstration.\")\n",
        "             # Choose an appropriate imputation strategy for your data\n",
        "             data_for_pca.fillna(data_for_pca.mean(), inplace=True)\n",
        "\n",
        "\n",
        "        # --- Apply PCA ---\n",
        "\n",
        "        # Decide on the number of components or the amount of variance to explain\n",
        "        # Option 1: Specify number of components (e.g., reduce to 10 features)\n",
        "        # n_components = 10\n",
        "        # pca = PCA(n_components=n_components)\n",
        "        # principal_components = pca.fit_transform(data_for_pca)\n",
        "        # print(f\"\\nApplied PCA to reduce dimensions to {n_components} components.\")\n",
        "\n",
        "        # Option 2: Specify variance to explain (e.g., keep components explaining 95% variance)\n",
        "        n_components_variance = 0.95\n",
        "        pca = PCA(n_components=n_components_variance)\n",
        "        principal_components = pca.fit_transform(data_for_pca)\n",
        "        print(f\"\\nApplied PCA to capture {n_components_variance*100:.0f}% variance.\")\n",
        "        print(f\"Number of components selected by PCA: {pca.n_components_}\")\n",
        "\n",
        "\n",
        "        # Create a new DataFrame with the principal components\n",
        "        pca_columns = [f'PC{i+1}' for i in range(pca.n_components_)]\n",
        "        principal_df = pd.DataFrame(data=principal_components, columns=pca_columns, index=df.index)\n",
        "\n",
        "        print(\"Principal Components DataFrame head:\")\n",
        "        display(principal_df.head())\n",
        "\n",
        "        # Optional: Visualize explained variance ratio\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "        plt.xlabel('Number of Components')\n",
        "        plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "        plt.title('PCA Cumulative Explained Variance')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        # Decide whether to replace original features with principal components\n",
        "        # If you replace, you need to drop the original features used for PCA\n",
        "        # and concatenate the principal components to your main DataFrame.\n",
        "\n",
        "        # Example: Replacing original features with principal components\n",
        "        print(\"\\nReplacing original features with principal components in the DataFrame...\")\n",
        "        # Drop the original features used for PCA\n",
        "        df_reduced = df.drop(columns=features_for_pca)\n",
        "\n",
        "        # Concatenate the principal components DataFrame\n",
        "        df_reduced = pd.concat([df_reduced, principal_df], axis=1)\n",
        "\n",
        "        # Update the main DataFrame reference\n",
        "        df = df_reduced\n",
        "\n",
        "        print(\"Original features replaced with principal components.\")\n",
        "        print(\"DataFrame head after dimensionality reduction:\")\n",
        "        display(df.head())\n",
        "        print(\"\\nDataFrame info after dimensionality reduction:\")\n",
        "        df.info()\n",
        "\n",
        "\n",
        "        # --- If PCA is primarily for Visualization ---\n",
        "        # If your goal was just to visualize, you might apply PCA to 2 or 3 components\n",
        "        # (pca = PCA(n_components=2) or PCA(n_components=3)) on a subset of your data\n",
        "        # (e.g., sample some data points if the dataset is huge) and then plot the components.\n",
        "        # This wouldn't necessarily change your main 'df' for modeling.\n",
        "\n",
        "\n",
        "    print(\"\\n--- Dimensionality Reduction Step Complete ---\")\n",
        "    print(\"Review the number of components and explained variance to decide if PCA is suitable.\")\n",
        "    print(\"If PCA was applied, your DataFrame 'df' now contains the principal components.\")\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame not loaded. Cannot perform dimensionality reduction.\")"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "# For time series data, splitting the data into training and testing sets is crucial\n",
        "# for evaluating your forecasting model's performance on unseen future data.\n",
        "# Unlike standard machine learning where you can randomly split data, you MUST\n",
        "# split time series data chronologically. You train on past data and predict future data.\n",
        "\n",
        "# Common splitting strategies for time series:\n",
        "# 1. Train/Test Split (fixed point): Split the data at a specific date or time index.\n",
        "#    Data before this point is for training, data after is for testing.\n",
        "# 2. Rolling Window (Walk-Forward Validation): Train on an initial period, test on the next period.\n",
        "#    Then, roll the training window forward (either fixing the window size or expanding it),\n",
        "#    and test on the subsequent period. Repeat multiple times. This is more robust\n",
        "#    for evaluating performance across different time periods.\n",
        "# 3. Expanding Window: Similar to rolling window, but the training set grows with each iteration,\n",
        "#    incorporating all data up to the start of the test period.\n",
        "\n",
        "# The splitting ratio (or the duration of train/test periods) should be chosen wisely:\n",
        "# - The training set needs to be long enough to capture relevant patterns (trends, seasonality).\n",
        "# - The test set should be representative of the period you want to forecast.\n",
        "# - Avoid making the test set too short, as performance metrics can be volatile.\n",
        "# - Avoid making the training set too short, as the model might not learn well.\n",
        "\n",
        "# Let's demonstrate a simple fixed point train/test split.\n",
        "# You'll need a time index for this, which we created in the \"Transform Your data\" step.\n",
        "\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "if df is not None:\n",
        "    print(\"--- Splitting Data (Time-Based) ---\")\n",
        "\n",
        "    # Ensure the DataFrame has a datetime index and is sorted by time\n",
        "    if not isinstance(df.index, pd.DatetimeIndex) or not df.index.is_monotonic_increasing:\n",
        "        print(\"Warning: DataFrame index is not a sorted DatetimeIndex.\")\n",
        "        print(\"Attempting to set 'Date' column as index and sort.\")\n",
        "        if 'Date' in df.columns:\n",
        "            try:\n",
        "                df['Date'] = pd.to_datetime(df['Date'])\n",
        "                df = df.set_index('Date').sort_index()\n",
        "                print(\"'Date' column set as index and sorted.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error setting 'Date' as index: {e}\")\n",
        "                print(\"Cannot perform time-based split without a valid datetime index.\")\n",
        "                df = None # Indicate that the DataFrame is not ready for splitting\n",
        "\n",
        "        else:\n",
        "            print(\"No 'Date' column found. Cannot perform time-based split.\")\n",
        "            df = None # Indicate that the DataFrame is not ready for splitting\n",
        "\n",
        "\n",
        "    if df is not None:\n",
        "        # --- Define Split Point ---\n",
        "\n",
        "        # You can define the split point by date or by index position.\n",
        "        # Splitting by date is generally more intuitive for time series.\n",
        "        # Choose a date that leaves a sufficient amount of data for both training and testing.\n",
        "\n",
        "        # Example: Split the last 20% of the data for testing.\n",
        "        # This requires sorting by date first.\n",
        "        train_size = 0.8 # 80% for training, 20% for testing\n",
        "\n",
        "        # Calculate the split index based on the ratio\n",
        "        split_index = int(len(df) * train_size)\n",
        "\n",
        "        # Get the split date based on the calculated index\n",
        "        split_date = df.index[split_index]\n",
        "\n",
        "        print(f\"\\nSplitting data at index: {split_index}\")\n",
        "        print(f\"Corresponding split date: {split_date}\")\n",
        "        print(f\"Training data will be from the start up to (but not including) {split_date}\")\n",
        "        print(f\"Testing data will be from {split_date} onwards\")\n",
        "\n",
        "\n",
        "        # --- Perform the Split ---\n",
        "\n",
        "        # Split the DataFrame based on the split date\n",
        "        train_df = df.loc[df.index < split_date].copy()\n",
        "        test_df = df.loc[df.index >= split_date].copy()\n",
        "\n",
        "        print(f\"\\nOriginal DataFrame shape: {df.shape}\")\n",
        "        print(f\"Training set shape: {train_df.shape}\")\n",
        "        print(f\"Testing set shape: {test_df.shape}\")\n",
        "\n",
        "        # Verify the time ranges\n",
        "        if not train_df.empty:\n",
        "             print(f\"Training set date range: {train_df.index.min()} to {train_df.index.max()}\")\n",
        "        if not test_df.empty:\n",
        "             print(f\"Testing set date range: {test_df.index.min()} to {test_df.index.max()}\")\n",
        "\n",
        "        # If your task is to forecast a specific column, separate features (X) and target (y)\n",
        "        # Decide your target variable here.\n",
        "        # Example: Assuming you are forecasting the count of incidents ('TYPE_Theft from Vehicle' if that's a one-hot encoded column)\n",
        "        # Or perhaps you need to aggregate data first if forecasting daily/weekly counts.\n",
        "\n",
        "        # --- Example: Preparing X and y (assuming a hypothetical target column) ---\n",
        "        # If your target is one of the columns in the DataFrame after transformations:\n",
        "        # target_column_name = 'TYPE_Theft from Vehicle' # Replace with your actual target column name\n",
        "        # if target_column_name in df.columns:\n",
        "        #     print(f\"\\nAssuming target variable is '{target_column_name}'.\")\n",
        "        #     X_train = train_df.drop(columns=[target_column_name])\n",
        "        #     y_train = train_df[target_column_name]\n",
        "        #     X_test = test_df.drop(columns=[target_column_name])\n",
        "        #     y_test = test_df[target_column_name]\n",
        "\n",
        "        #     print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "        #     print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "        # else:\n",
        "        #     print(f\"\\nTarget column '{target_column_name}' not found. Skipping X/y split.\")\n",
        "        #     print(\"Please define your target variable and perform the X/y split accordingly.\")\n",
        "\n",
        "        # If your target is an aggregated series (e.g., daily crime count), you would\n",
        "        # perform the aggregation *before* or *after* the initial df split, ensuring\n",
        "        # the resulting series/DataFrame is aligned with the train/test periods.\n",
        "\n",
        "        # For now, we'll just keep the train_df and test_df DataFrames as the split result.\n",
        "        # You will extract X and y or prepare the data further based on your specific model requirements.\n",
        "\n",
        "        print(\"\\n--- Data Splitting Complete ---\")\n",
        "        print(\"You have train_df and test_df ready for time series modeling.\")\n",
        "        print(\"Remember to define and separate your target variable(s) before training.\")\n",
        "\n",
        "    else:\n",
        "        print(\"DataFrame not available for splitting.\")\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame not loaded. Cannot split data.\")"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter # To check class distribution\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "if 'train_df' in locals() and 'test_df' in locals() and train_df is not None and test_df is not None:\n",
        "    print(\"--- Implementing SARIMA Model (Example: Forecasting Total Daily Crime Count) ---\")\n",
        "    train_ts = train_df.resample('D').size().fillna(0)\n",
        "    test_ts = test_df.resample('D').size().fillna(0)\n",
        "\n",
        "    print(f\"\\nTraining time series shape: {train_ts.shape}\")\n",
        "    print(f\"Testing time series shape: {test_ts.shape}\")\n",
        "    print(\"\\nTraining time series head:\")\n",
        "    display(train_ts.head())\n",
        "    print(\"\\nTesting time series head:\")\n",
        "    display(test_ts.head())\n",
        "\n",
        "    order = (1, 1, 1)\n",
        "    seasonal_order = (1, 1, 1, 7)\n",
        "\n",
        "    print(f\"\\nFitting SARIMA model with order={order} and seasonal_order={seasonal_order}...\")\n",
        "\n",
        "    try:\n",
        "\n",
        "        model = SARIMAX(train_ts,\n",
        "                        order=order,\n",
        "                        seasonal_order=seasonal_order,\n",
        "                        enforce_stationarity=False,\n",
        "                        enforce_invertibility=False)\n",
        "        sarima_results = model.fit()\n",
        "\n",
        "        print(\"\\nSARIMA model fitted successfully.\")\n",
        "        print(sarima_results.summary())\n",
        "        print(\"\\nMaking predictions on the test set...\")\n",
        "        start_pred = test_ts.index[0]\n",
        "        end_pred = test_ts.index[-1]\n",
        "        predictions = sarima_results.predict(start=start_pred, end=end_pred, dynamic=True)\n",
        "\n",
        "        print(\"Predictions generated.\")\n",
        "        print(\"\\nSample Predictions:\")\n",
        "        print(predictions.head())\n",
        "        print(\"\\nSample Actuals (Test set):\")\n",
        "        print(test_ts.head())\n",
        "        predictions_aligned = predictions.reindex(test_ts.index)\n",
        "        predictions_aligned = predictions_aligned.fillna(method='ffill')\n",
        "        print(\"\\nEvaluating model performance on the test set...\")\n",
        "        valid_indices = predictions_aligned.notna() & test_ts.notna()\n",
        "        actuals = test_ts[valid_indices]\n",
        "        preds = predictions_aligned[valid_indices]\n",
        "\n",
        "        if not actuals.empty:\n",
        "            rmse = np.sqrt(mean_squared_error(actuals, preds))\n",
        "            mae = mean_absolute_error(actuals, preds)\n",
        "\n",
        "            print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "            print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "            plt.figure(figsize=(15, 6))\n",
        "            plt.plot(train_ts.index, train_ts, label='Training Data')\n",
        "            plt.plot(test_ts.index, actuals, label='Actual Test Data', color='orange')\n",
        "            plt.plot(preds.index, preds, label='SARIMA Predictions', color='green', linestyle='--')\n",
        "            plt.title('SARIMA Forecast vs Actuals (Daily Crime Count)')\n",
        "            plt.xlabel('Date')\n",
        "            plt.ylabel('Number of Incidents')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "        else:\n",
        "             print(\"No valid data points to evaluate predictions against in the test set.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during SARIMA model fitting or prediction: {e}\")\n",
        "        print(\"Consider checking data stationarity, choosing different SARIMA parameters, or trying a different model.\")\n",
        "        print(\"If using exogenous variables, ensure they are aligned correctly and have no NaNs.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Training and/or Testing DataFrames not found or are None.\")\n",
        "    print(\"Please ensure the data splitting step ran successfully.\")"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "try:\n",
        "    sarima_rmse = rmse\n",
        "    sarima_mae = mae\n",
        "    metrics_calculated = True\n",
        "except NameError:\n",
        "    print(\"RMSE and MAE variables not found from the previous model training step.\")\n",
        "    print(\"Skipping evaluation metric visualization.\")\n",
        "    metrics_calculated = False\n",
        "if metrics_calculated:\n",
        "    print(\"--- Visualizing Evaluation Metric Scores ---\")\n",
        "    metrics_data = {'Metric': ['RMSE', 'MAE'],\n",
        "                    'SARIMA Model': [sarima_rmse, sarima_mae]}\n",
        "    metrics_df = pd.DataFrame(metrics_data)\n",
        "    metrics_df = metrics_df.set_index('Metric')\n",
        "    ax = metrics_df.plot(kind='bar', figsize=(8, 6), colormap='viridis')\n",
        "\n",
        "    plt.title('Model Evaluation Metrics on Test Set')\n",
        "    plt.ylabel('Score Value')\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.grid(axis='y', linestyle='--')\n",
        "    plt.legend(title='Model')\n",
        "    for container in ax.containers:\n",
        "        ax.bar_label(container, fmt='%.2f')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n--- Evaluation Metric Score Chart Displayed ---\")\n",
        "    print(\"This chart compares the RMSE and MAE of the SARIMA model on the test set.\")\n",
        "    print(\"Add results from other models to this chart for comparison.\")\n",
        "\n",
        "else:\n",
        "    print(\"Cannot generate evaluation metric chart as scores were not calculated.\")"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall pmdarima and numpy\n",
        "!pip uninstall pmdarima -y\n",
        "!pip uninstall numpy -y\n",
        "!pip install numpy\n",
        "!pip install pmdarima\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "if 'train_ts' in locals() and 'test_ts' in locals() and train_ts is not None and test_ts is not None:\n",
        "    print(\"--- Implementing SARIMA Model with Hyperparameter Optimization (Auto-ARIMA) ---\")\n",
        "    train_ts_clean = train_ts.dropna()\n",
        "    test_ts_clean = test_ts.dropna()\n",
        "\n",
        "    if train_ts_clean.empty or test_ts_clean.empty:\n",
        "        print(\"Cleaned training or testing time series is empty after dropping NaNs. Cannot proceed.\")\n",
        "    else:\n",
        "        print(f\"Cleaned training time series shape: {train_ts_clean.shape}\")\n",
        "        print(f\"Cleaned testing time series shape: {test_ts_clean.shape}\")\n",
        "        print(\"\\nRunning auto_arima to find best SARIMA parameters...\")\n",
        "        try:\n",
        "            arima_model = pm.auto_arima(train_ts_clean,\n",
        "                                       start_p=1, start_q=1,\n",
        "                                       max_p=5, max_q=5,\n",
        "                                       start_P=1, start_Q=1,\n",
        "                                       max_P=3, max_Q=3,\n",
        "                                       m=7,\n",
        "                                       seasonal=True,\n",
        "                                       d=None, D=None,\n",
        "                                       trace=True,\n",
        "                                       error_action='ignore',\n",
        "                                       suppress_warnings=True,\n",
        "                                       stepwise=True)\n",
        "\n",
        "\n",
        "            print(\"\\nauto_arima search complete.\")\n",
        "            print(f\"Best SARIMA parameters found: {arima_model.order} (non-seasonal) and {arima_model.seasonal_order} (seasonal)\")\n",
        "            print(arima_model.summary())\n",
        "            best_order = arima_model.order\n",
        "            best_seasonal_order = arima_model.seasonal_order\n",
        "\n",
        "            print(\"\\nFitting SARIMA model using the best parameters found by auto_arima...\")\n",
        "            model = SARIMAX(train_ts_clean,\n",
        "                            order=best_order,\n",
        "                            seasonal_order=best_seasonal_order,\n",
        "                            enforce_stationarity=False,\n",
        "                            enforce_invertibility=False)\n",
        "\n",
        "            sarima_optimized_results = model.fit()\n",
        "\n",
        "            print(\"\\nSARIMA model fitted successfully with optimized parameters.\")\n",
        "            print(sarima_optimized_results.summary())\n",
        "\n",
        "            print(\"\\nMaking predictions on the test set using the optimized model...\")\n",
        "\n",
        "            start_pred = test_ts_clean.index[0]\n",
        "            end_pred = test_ts_clean.index[-1]\n",
        "            predictions_optimized = sarima_optimized_results.predict(start=start_pred, end=end_pred, dynamic=True)\n",
        "\n",
        "            print(\"Predictions generated.\")\n",
        "            print(\"\\nSample Predictions (Optimized Model):\")\n",
        "            print(predictions_optimized.head())\n",
        "            print(\"\\nSample Actuals (Cleaned Test set):\")\n",
        "            print(test_ts_clean.head())\n",
        "\n",
        "            # Ensure predictions and actuals have the same index for evaluation\n",
        "            predictions_optimized_aligned = predictions_optimized.reindex(test_ts_clean.index)\n",
        "\n",
        "            # Fill any NaNs in predictions_optimized_aligned\n",
        "            predictions_optimized_aligned = predictions_optimized_aligned.fillna(method='ffill') # Or use 0, or other method\n",
        "\n",
        "            # --- Evaluate the Optimized Model ---\n",
        "            print(\"\\nEvaluating optimized model performance on the test set...\")\n",
        "\n",
        "            # Ensure no NaNs or infinities before calculating metrics\n",
        "            valid_indices_optimized = predictions_optimized_aligned.notna() & test_ts_clean.notna()\n",
        "            actuals_optimized = test_ts_clean[valid_indices_optimized]\n",
        "            preds_optimized = predictions_optimized_aligned[valid_indices_optimized]\n",
        "\n",
        "            if not actuals_optimized.empty:\n",
        "                rmse_optimized = np.sqrt(mean_squared_error(actuals_optimized, preds_optimized))\n",
        "                mae_optimized = mean_absolute_error(actuals_optimized, preds_optimized)\n",
        "\n",
        "                print(f\"Optimized Root Mean Squared Error (RMSE): {rmse_optimized:.2f}\")\n",
        "                print(f\"Optimized Mean Absolute Error (MAE): {mae_optimized:.2f}\")\n",
        "\n",
        "                # Store optimized scores to compare later\n",
        "                optimized_sarima_rmse = rmse_optimized\n",
        "                optimized_sarima_mae = mae_optimized\n",
        "                optimized_metrics_calculated = True\n",
        "\n",
        "\n",
        "                # --- Visualize Forecast vs Actuals (Optimized) ---\n",
        "                plt.figure(figsize=(15, 6))\n",
        "                plt.plot(train_ts_clean.index, train_ts_clean, label='Training Data')\n",
        "                plt.plot(test_ts_clean.index, actuals_optimized, label='Actual Test Data', color='orange')\n",
        "                plt.plot(preds_optimized.index, preds_optimized, label='Optimized SARIMA Predictions', color='green', linestyle='--')\n",
        "                plt.title('Optimized SARIMA Forecast vs Actuals (Daily Crime Count)')\n",
        "                plt.xlabel('Date')\n",
        "                plt.ylabel('Number of Incidents')\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "                plt.show()\n",
        "\n",
        "            else:\n",
        "                 print(\"No valid data points to evaluate optimized predictions against in the test set.\")\n",
        "                 optimized_metrics_calculated = False\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nAn error occurred during auto_arima or optimized SARIMA fitting/prediction: {e}\")\n",
        "            print(\"Consider adjusting auto_arima parameters (max_order, m, stepwise=False), or checking data for issues.\")\n",
        "            optimized_metrics_calculated = False\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Training and/or Testing Time Series not found or are None.\")\n",
        "    print(\"Please ensure the data aggregation and splitting steps ran successfully.\")"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    # Try to use the scores from the optimized model\n",
        "    sarima_rmse_score = optimized_sarima_rmse\n",
        "    sarima_mae_score = optimized_sarima_mae\n",
        "    metrics_calculated = True\n",
        "    print(\"Using evaluation metrics from the optimized SARIMA model.\")\n",
        "except NameError:\n",
        "    try:\n",
        "        # Fallback to the scores from the initial SARIMA model if optimized scores are not available\n",
        "        sarima_rmse_score = rmse\n",
        "        sarima_mae_score = mae\n",
        "        metrics_calculated = True\n",
        "        print(\"Using evaluation metrics from the initial SARIMA model.\")\n",
        "    except NameError:\n",
        "        print(\"RMSE and MAE variables not found from the previous model training steps.\")\n",
        "        print(\"Skipping evaluation metric visualization.\")\n",
        "        metrics_calculated = False\n",
        "\n",
        "\n",
        "if metrics_calculated:\n",
        "    print(\"--- Visualizing Evaluation Metric Scores ---\")\n",
        "\n",
        "    # Create a DataFrame to hold the metrics for plotting\n",
        "    metrics_data = {'Metric': ['RMSE', 'MAE'],\n",
        "                    'SARIMA Model': [sarima_rmse_score, sarima_mae_score]}\n",
        "                    # Add more models here as you implement them\n",
        "                    # 'LightGBM Model': [lightgbm_rmse, lightgbm_mae]}\n",
        "\n",
        "    metrics_df = pd.DataFrame(metrics_data)\n",
        "\n",
        "    # Set the 'Metric' column as the index for easier plotting\n",
        "    metrics_df = metrics_df.set_index('Metric')\n",
        "\n",
        "    # Plot the bar chart\n",
        "    ax = metrics_df.plot(kind='bar', figsize=(8, 6), colormap='viridis')\n",
        "\n",
        "    plt.title('Model Evaluation Metrics on Test Set')\n",
        "    plt.ylabel('Score Value')\n",
        "    plt.xticks(rotation=0) # Keep x-axis labels horizontal\n",
        "    plt.grid(axis='y', linestyle='--')\n",
        "    plt.legend(title='Model')\n",
        "\n",
        "    # Add the score values on top of the bars\n",
        "    for container in ax.containers:\n",
        "        ax.bar_label(container, fmt='%.2f') # Format to 2 decimal places\n",
        "\n",
        "    plt.tight_layout() # Adjust layout\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n--- Evaluation Metric Score Chart Displayed ---\")\n",
        "    print(\"This chart compares the RMSE and MAE of the SARIMA model on the test set.\")\n",
        "    print(\"Add results from other models to this chart for comparison.\")\n",
        "\n",
        "else:\n",
        "    print(\"Cannot generate evaluation metric chart as scores were not calculated.\")"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall pmdarima\n",
        "!pip uninstall pmdarima -y\n",
        "\n",
        "# Reinstall pmdarima\n",
        "!pip install pmdarima\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "if 'train_ts' in locals() and 'test_ts' in locals() and train_ts is not None and test_ts is not None:\n",
        "    print(\"--- Implementing SARIMA Model with Hyperparameter Optimization (Auto-ARIMA) ---\")\n",
        "\n",
        "    # Ensure there are no NaNs in the time series used for auto_arima\n",
        "    # Drop NaNs primarily for the time series used in auto_arima and model fitting/prediction.\n",
        "    train_ts_clean = train_ts.dropna()\n",
        "    test_ts_clean = test_ts.dropna()\n",
        "\n",
        "    if train_ts_clean.empty or test_ts_clean.empty:\n",
        "        print(\"Cleaned training or testing time series is empty after dropping NaNs. Cannot proceed with modeling.\")\n",
        "    else:\n",
        "        print(f\"Cleaned training time series shape: {train_ts_clean.shape}\")\n",
        "        print(f\"Cleaned testing time series shape: {test_ts_clean.shape}\")\n",
        "\n",
        "        # --- Hyperparameter Optimization using auto_arima ---\n",
        "\n",
        "        print(\"\\nRunning auto_arima to find best SARIMA parameters...\")\n",
        "\n",
        "        try:\n",
        "\n",
        "            arima_model = pm.auto_arima(train_ts_clean,\n",
        "                                       start_p=1, start_q=1,\n",
        "                                       max_p=5, max_q=5, # Limit search space for non-seasonal\n",
        "                                       start_P=1, start_Q=1,\n",
        "                                       max_P=3, max_Q=3, # Limit search space for seasonal\n",
        "                                       m=7,            # Seasonal period (e.g., 7 for weekly daily data)\n",
        "                                       seasonal=True,\n",
        "                                       d=None, D=None, # Let auto_arima find differencing order\n",
        "                                       trace=True,      # Print status updates\n",
        "                                       error_action='ignore', # Ignore errors for specific parameter combinations\n",
        "                                       suppress_warnings=True, # Hide convergence warnings\n",
        "                                       stepwise=True) # Use stepwise search (faster)\n",
        "                                       # n_fits=10 # Number of random fits for random search (if stepwise=False)\n",
        "\n",
        "\n",
        "            print(\"\\nauto_arima search complete.\")\n",
        "            print(f\"Best SARIMA parameters found: {arima_model.order} (non-seasonal) and {arima_model.seasonal_order} (seasonal)\")\n",
        "            print(arima_model.summary())\n",
        "\n",
        "            # The arima_model object is itself a fitted model, ready for prediction.\n",
        "            # We can directly use this model for prediction.\n",
        "\n",
        "            # --- Predict on the model ---\n",
        "\n",
        "            # Make predictions on the test set period\n",
        "            print(\"\\nMaking predictions on the test set using the auto_arima optimized model...\")\n",
        "            n_periods = len(test_ts_clean)\n",
        "            predictions_optimized = arima_model.predict(n_periods=n_periods)\n",
        "\n",
        "            # auto_arima predict returns a numpy array. Convert to pandas Series with test index for evaluation.\n",
        "            predictions_optimized_series = pd.Series(predictions_optimized, index=test_ts_clean.index)\n",
        "\n",
        "\n",
        "            print(\"Predictions generated.\")\n",
        "            print(\"\\nSample Predictions (Optimized Model):\")\n",
        "            print(predictions_optimized_series.head())\n",
        "            print(\"\\nSample Actuals (Cleaned Test set):\")\n",
        "            print(test_ts_clean.head())\n",
        "\n",
        "            # --- Evaluate the Optimized Model ---\n",
        "            print(\"\\nEvaluating optimized model performance on the test set...\")\n",
        "\n",
        "            # Ensure no NaNs or infinities before calculating metrics\n",
        "            # The predictions_optimized_series should align with test_ts_clean by index.\n",
        "            # We can drop NaNs just in case, though it shouldn't be necessary if indices match.\n",
        "            valid_indices_optimized = predictions_optimized_series.notna() & test_ts_clean.notna()\n",
        "            actuals_optimized = test_ts_clean[valid_indices_optimized]\n",
        "            preds_optimized = predictions_optimized_series[valid_indices_optimized]\n",
        "\n",
        "            if not actuals_optimized.empty:\n",
        "                rmse_optimized = np.sqrt(mean_squared_error(actuals_optimized, preds_optimized))\n",
        "                mae_optimized = mean_absolute_error(actuals_optimized, preds_optimized)\n",
        "\n",
        "                print(f\"Optimized Root Mean Squared Error (RMSE): {rmse_optimized:.2f}\")\n",
        "                print(f\"Optimized Mean Absolute Error (MAE): {mae_optimized:.2f}\")\n",
        "\n",
        "                # Store optimized scores to compare later\n",
        "                optimized_sarima_rmse = rmse_optimized\n",
        "                optimized_sarima_mae = mae_optimized\n",
        "                # Make these available globally for the evaluation chart cell\n",
        "                get_ipython().push({'optimized_sarima_rmse': optimized_sarima_rmse,\n",
        "                                    'optimized_sarima_mae': optimized_sarima_mae})\n",
        "                optimized_metrics_calculated = True\n",
        "\n",
        "\n",
        "                # --- Visualize Forecast vs Actuals (Optimized) ---\n",
        "                plt.figure(figsize=(15, 6))\n",
        "                plt.plot(train_ts_clean.index, train_ts_clean, label='Training Data')\n",
        "                plt.plot(test_ts_clean.index, actuals_optimized, label='Actual Test Data', color='orange')\n",
        "                plt.plot(preds_optimized.index, preds_optimized, label='Optimized SARIMA Predictions', color='green', linestyle='--')\n",
        "                plt.title('Optimized SARIMA Forecast vs Actuals (Daily Crime Count)')\n",
        "                plt.xlabel('Date')\n",
        "                plt.ylabel('Number of Incidents')\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "                plt.show()\n",
        "\n",
        "            else:\n",
        "                 print(\"No valid data points to evaluate optimized predictions against in the test set.\")\n",
        "                 optimized_metrics_calculated = False\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nAn error occurred during auto_arima fitting or prediction: {e}\")\n",
        "            print(\"Consider adjusting auto_arima parameters (max_order, m, stepwise=False), or checking data for issues.\")\n",
        "            # If auto_arima fails, the optimized metrics are not calculated\n",
        "            optimized_metrics_calculated = False\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Training and/or Testing Time Series not found or are None.\")\n",
        "    print(\"Please ensure the data aggregation and splitting steps ran successfully before this cell.\")"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall existing statsmodels and numpy aggressively\n",
        "!pip uninstall statsmodels -y\n",
        "!pip uninstall numpy -y\n",
        "\n",
        "# Clear pip cache\n",
        "!pip cache purge\n",
        "\n",
        "# Install compatible versions of statsmodels and numpy\n",
        "# Explicitly specify a statsmodels version known to be compatible with recent numpy versions\n",
        "!pip install statsmodels numpy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "if 'daily_crime_counts' in globals() and daily_crime_counts is not None and len(daily_crime_counts) > 100:\n",
        "    print(\"Using the prepared daily_crime_counts time series for Holt-Winters.\")\n",
        "    train_size = int(len(daily_crime_counts) * 0.8)\n",
        "    train_data, test_data = daily_crime_counts[0:train_size], daily_crime_counts[train_size:]\n",
        "\n",
        "    alphas = [0.2, 0.4, 0.6, 0.8]\n",
        "    betas = [0.1, 0.3, 0.5]\n",
        "    gammas = [0.1, 0.3, 0.5, 0.7]\n",
        "    seasonal_modes = ['add', 'mul']\n",
        "    seasonal_period = 7\n",
        "\n",
        "    best_rmse = float('inf')\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "\n",
        "\n",
        "    trend_modes = ['add', 'mul']\n",
        "    seasonal_periods_to_test = [7, 30]\n",
        "\n",
        "\n",
        "    print(\"\\nPerforming Grid Search for Holt-Winters model types and seasonal periods...\")\n",
        "    best_rmse = float('inf')\n",
        "    best_model_params = None\n",
        "    best_fitted_model = None\n",
        "\n",
        "    for trend_mode in trend_modes:\n",
        "        for seasonal_mode in seasonal_modes:\n",
        "            for period in seasonal_periods_to_test:\n",
        "                 try:\n",
        "                    if len(train_data) < period * 2:\n",
        "                        print(f\"Skipping period {period} as data is too short.\")\n",
        "                        continue\n",
        "\n",
        "                    print(f\"Trying params: trend='{trend_mode}', seasonal='{seasonal_mode}', seasonal_periods={period}\")\n",
        "                    model = ExponentialSmoothing(train_data,\n",
        "                                                 trend=trend_mode,\n",
        "                                                 seasonal=seasonal_mode,\n",
        "                                                 seasonal_periods=period,\n",
        "                                                 initialization_method='estimated')\n",
        "                    fitted_model = model.fit()\n",
        "                    predictions = fitted_model.predict(start=test_data.index[0], end=test_data.index[-1])\n",
        "                    rmse = mean_squared_error(test_data, predictions, squared=False)\n",
        "\n",
        "                    print(f\"Params: trend='{trend_mode}', seasonal='{seasonal_mode}', seasonal_periods={period}, RMSE: {rmse}\")\n",
        "                    if rmse < best_rmse:\n",
        "                        best_rmse = rmse\n",
        "                        best_model_params = {\n",
        "                            'trend': trend_mode,\n",
        "                            'seasonal': seasonal_mode,\n",
        "                            'seasonal_periods': period\n",
        "                        }\n",
        "                        best_fitted_model = fitted_model # Store the fitted model\n",
        "\n",
        "\n",
        "                 except Exception as e:\n",
        "                    print(f\"Error fitting model with params trend='{trend_mode}', seasonal='{seasonal_mode}', seasonal_periods={period}: {e}\")\n",
        "                    continue\n",
        "\n",
        "    print(\"\\nHolt-Winters Grid Search completed.\")\n",
        "\n",
        "    if best_model_params:\n",
        "        print(\"Best Holt-Winters Model Parameters:\", best_model_params)\n",
        "        print(\"Best RMSE:\", best_rmse)\n",
        "        print(\"\\nFitting the final Holt-Winters model with best parameters on the full dataset...\")\n",
        "        try:\n",
        "            final_model = ExponentialSmoothing(daily_crime_counts,\n",
        "                                              trend=best_model_params['trend'],\n",
        "                                              seasonal=best_model_params['seasonal'],\n",
        "                                              seasonal_periods=best_model_params['seasonal_periods'],\n",
        "                                              initialization_method='estimated')\n",
        "\n",
        "            final_fitted_model = final_model.fit()\n",
        "            start_date_future = daily_crime_counts.index[-1] + pd.Timedelta(days=1)\n",
        "            end_date_future = start_date_future + pd.Timedelta(days=29)\n",
        "            forecast_future = final_fitted_model.predict(start=start_date_future, end=end_date_future)\n",
        "\n",
        "            print(\"\\nFuture forecast generated.\")\n",
        "            print(forecast_future) # Display the future predictions\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fitting final model or generating forecast: {e}\")\n",
        "    else:\n",
        "        print(\"\\nNo valid Holt-Winters model parameters found during grid search.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping ML Model implementation as the daily_crime_counts time series was not prepared.\")\n",
        "    print(\"Please ensure the data preparation steps were run.\")"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# You would typically generate visualizations of the model's performance here.\n",
        "# Since the previous cell performs a grid search and finds the best RMSE,\n",
        "# you can display this result or create a bar chart comparing RMSEs for different models\n",
        "# if you were comparing multiple approaches (like Prophet and SARIMA).\n",
        "\n",
        "# As you have only implemented SARIMA so far, we can display the best RMSE found.\n",
        "\n",
        "if 'best_rmse' in globals() and best_rmse != float('inf'):\n",
        "    print(f\"Best RMSE found during SARIMA Grid Search: {best_rmse:.4f}\")\n",
        "\n",
        "    # You could create a simple bar chart if you had multiple models or variations to compare\n",
        "    # Example (if comparing SARIMA and Prophet's best RMSE):\n",
        "    # model_names = ['Best SARIMA', 'Best Prophet'] # Assuming you have a best_prophet_rmse\n",
        "    # rmse_scores = [best_rmse, best_prophet_rmse] # Replace with actual best RMSEs\n",
        "\n",
        "    # plt.figure(figsize=(8, 5))\n",
        "    # sns.barplot(x=model_names, y=rmse_scores, palette='viridis')\n",
        "    # plt.title('Comparison of Model RMSE Scores')\n",
        "    # plt.ylabel('Root Mean Squared Error (RMSE)')\n",
        "    # plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"No best RMSE found. Please run the model fitting code first.\")"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Import necessary libraries if not already imported\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# Make sure you have run the pip installs from the previous fix before this cell\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Assume 'daily_crime_counts' time series is prepared and available\n",
        "# Based on the previous code, the check is already in place\n",
        "\n",
        "if 'daily_crime_counts' in globals() and daily_crime_counts is not None and len(daily_crime_counts) > 100:\n",
        "    print(\"Using the prepared daily_crime_counts time series for Holt-Winters.\")\n",
        "\n",
        "    # Split data into training and testing sets (80/20 split)\n",
        "    train_size = int(len(daily_crime_counts) * 0.8)\n",
        "    train_data, test_data = daily_crime_counts[0:train_size], daily_crime_counts[train_size:]\n",
        "\n",
        "    # Define parameter grids for Grid Search\n",
        "    # Note: alphas, betas, gammas are smoothing parameters which can also be tuned,\n",
        "    # but statsmodels can estimate these during fitting. Tuning trend, seasonal\n",
        "    # and seasonal_periods is a common initial approach.\n",
        "    trend_modes = ['add', 'mul']\n",
        "    seasonal_modes = ['add', 'mul']\n",
        "    # Test common seasonal periods: 7 for weekly seasonality, 30 for monthly approximation\n",
        "    seasonal_periods_to_test = [7, 30] # Can add more periods like 365 for yearly if data span allows\n",
        "\n",
        "    best_rmse = float('inf') # Initialize with a high value\n",
        "    best_model_params = None # To store the parameters that yield the lowest RMSE\n",
        "    best_fitted_model = None # To store the best fitted model from the test set\n",
        "\n",
        "    print(\"\\nPerforming Grid Search for Holt-Winters model types and seasonal periods...\")\n",
        "\n",
        "    # Iterate through all combinations of parameters\n",
        "    for trend_mode in trend_modes:\n",
        "        for seasonal_mode in seasonal_modes:\n",
        "            for period in seasonal_periods_to_test:\n",
        "                 # Skip combinations that might cause issues or are not suitable\n",
        "                 # For example, 'mul' seasonal requires non-zero values, and data needs enough points\n",
        "                 if seasonal_mode == 'mul' and (train_data <= 0).any():\n",
        "                     print(f\"Skipping seasonal='mul' due to zero or negative values in data.\")\n",
        "                     continue\n",
        "                 if len(train_data) < period * 2:\n",
        "                     print(f\"Skipping period {period} as training data ({len(train_data)} points) is too short (need at least 2*period).\")\n",
        "                     continue\n",
        "\n",
        "                 try:\n",
        "                    print(f\"Trying params: trend='{trend_mode}', seasonal='{seasonal_mode}', seasonal_periods={period}\")\n",
        "                    # Define the Exponential Smoothing model with current parameters\n",
        "                    model = ExponentialSmoothing(train_data,\n",
        "                                                 trend=trend_mode,\n",
        "                                                 seasonal=seasonal_mode,\n",
        "                                                 seasonal_periods=period,\n",
        "                                                 initialization_method='estimated') # Let statsmodels estimate initial values\n",
        "\n",
        "                    # Fit the model to the training data\n",
        "                    fitted_model = model.fit()\n",
        "\n",
        "                    # Predict on the test data\n",
        "                    # Ensure the prediction range matches the test data index\n",
        "                    predictions = fitted_model.predict(start=test_data.index[0], end=test_data.index[-1])\n",
        "\n",
        "                    # Calculate RMSE for the test data\n",
        "                    rmse = mean_squared_error(test_data, predictions, squared=False) # squared=False gives RMSE\n",
        "\n",
        "                    print(f\"Params: trend='{trend_mode}', seasonal='{seasonal_mode}', seasonal_periods={period}, RMSE: {rmse}\")\n",
        "\n",
        "                    # Check if the current model is the best so far\n",
        "                    if rmse < best_rmse:\n",
        "                        best_rmse = rmse\n",
        "                        best_model_params = {\n",
        "                            'trend': trend_mode,\n",
        "                            'seasonal': seasonal_mode,\n",
        "                            'seasonal_periods': period\n",
        "                        }\n",
        "                        # Optionally store the fitted model, though we'll refit on full data later\n",
        "                        # best_fitted_model = fitted_model\n",
        "\n",
        "\n",
        "                 except Exception as e:\n",
        "                    # Catch potential errors during fitting (e.g., convergence issues)\n",
        "                    print(f\"Error fitting model with params trend='{trend_mode}', seasonal='{seasonal_mode}', seasonal_periods={period}: {e}\")\n",
        "                    continue\n",
        "\n",
        "    print(\"\\nHolt-Winters Grid Search completed.\")\n",
        "\n",
        "    # Fit the final model using the best parameters found on the full dataset\n",
        "    if best_model_params:\n",
        "        print(\"Best Holt-Winters Model Parameters:\", best_model_params)\n",
        "        print(\"Best RMSE on Test Data:\", best_rmse) # This is the RMSE from the test set evaluation\n",
        "\n",
        "        print(\"\\nFitting the final Holt-Winters model with best parameters on the full dataset...\")\n",
        "        try:\n",
        "            # Define the final model using the entire time series and best parameters\n",
        "            final_model = ExponentialSmoothing(daily_crime_counts,\n",
        "                                              trend=best_model_params['trend'],\n",
        "                                              seasonal=best_model_params['seasonal'],\n",
        "                                              seasonal_periods=best_model_params['seasonal_periods'],\n",
        "                                              initialization_method='estimated')\n",
        "\n",
        "            # Fit the model to the full dataset\n",
        "            final_fitted_model = final_model.fit()\n",
        "\n",
        "            print(\"Final model fitted on the full dataset.\")\n",
        "\n",
        "            # Predict into the future (e.g., next 30 days)\n",
        "            # Determine the start and end dates for the future forecast\n",
        "            start_date_future = daily_crime_counts.index[-1] + pd.Timedelta(days=1)\n",
        "            # Predict for the next 30 days\n",
        "            end_date_future = start_date_future + pd.Timedelta(days=29) # This is the end date inclusive\n",
        "\n",
        "            # Generate future predictions\n",
        "            forecast_future = final_fitted_model.predict(start=start_date_future, end=end_date_future)\n",
        "\n",
        "            print(\"\\nFuture forecast generated (next 30 days):\")\n",
        "            print(forecast_future) # Display the future predictions\n",
        "\n",
        "            # Chart visualization code\n",
        "            plt.figure(figsize=(15, 7))\n",
        "            plt.plot(daily_crime_counts.index, daily_crime_counts, label='Observed Data')\n",
        "            plt.plot(test_data.index, predictions, label='Test Set Predictions', color='orange', linestyle='--')\n",
        "            plt.plot(forecast_future.index, forecast_future, label='Future Forecast', color='green')\n",
        "            plt.title('Holt-Winters Forecasting of Daily Crime Counts')\n",
        "            plt.xlabel('Date')\n",
        "            plt.ylabel('Number of Incidents')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fitting final model or generating forecast: {e}\")\n",
        "    else:\n",
        "        print(\"\\nNo valid Holt-Winters model parameters found during grid search. Cannot fit final model or forecast.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping ML Model implementation as the daily_crime_counts time series was not prepared or is too short.\")\n",
        "    print(\"Please ensure the data preparation steps were run and the time series has enough data points (>100).\")"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import pickle\n",
        "# Import any necessary libraries for your model (e.g., sklearn, tensorflow, pytorch)\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from your_forecasting_model import YourModel\n",
        "\n",
        "# --- Placeholder for Model Training or Loading ---\n",
        "# You need to define or load your 'model' variable here.\n",
        "# For example, if you were training a model:\n",
        "# model = YourModel(...) # Initialize your model\n",
        "# model.fit(X_train, y_train) # Train your model on training data\n",
        "\n",
        "# Or if you were loading a model:\n",
        "# with open(\"path/to/your/trained_model.pkl\", \"rb\") as infile:\n",
        "#     model = pickle.load(infile)\n",
        "# --- End of Placeholder ---\n",
        "\n",
        "# Example: Creating a dummy model object for demonstration purposes if you don't have a real model yet\n",
        "# REMOVE this section once you have your actual model training/loading code\n",
        "class DummyModel:\n",
        "    def predict(self, X):\n",
        "        print(\"This is a dummy predict method.\")\n",
        "        return [0] * len(X)\n",
        "\n",
        "model = DummyModel()\n",
        "# END of Dummy Model Example - Please replace with your actual model\n",
        "\n",
        "\n",
        "# Save the model\n",
        "try:\n",
        "    with open(\"best_model.pkl\", \"wb\") as file:\n",
        "        pickle.dump(model, file)\n",
        "    print(\"Model saved successfully to best_model.pkl\")\n",
        "except NameError:\n",
        "    print(\"Error: 'model' is not defined. Please ensure you have trained or loaded a model into a variable named 'model'.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while saving the model: {e}\")\n",
        "\n",
        "\n",
        "# Load the model later\n",
        "try:\n",
        "    with open(\"best_model.pkl\", \"rb\") as file:\n",
        "        loaded_model = pickle.load(file)\n",
        "    print(\"Model loaded successfully from best_model.pkl\")\n",
        "    # You can now use loaded_model\n",
        "    # For example: loaded_model.predict(...)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'best_model.pkl' not found. Please ensure the save operation was successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the model: {e}\")"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "import joblib\n",
        "import pickle # Although not used for joblib, it was in the original context\n",
        "\n",
        "# --- Placeholder for Model Training or Loading ---\n",
        "# You need to define or load your 'model' variable here.\n",
        "# For example, if you were training a model:\n",
        "# model = YourModel(...) # Initialize your model\n",
        "# model.fit(X_train, y_train) # Train your model on training data\n",
        "\n",
        "# Or if you were loading a model:\n",
        "# with open(\"path/to/your/trained_model.pkl\", \"rb\") as infile:\n",
        "#     model = pickle.load(infile) # Or joblib.load\n",
        "\n",
        "# Example: Creating a dummy model object for demonstration purposes if you don't have a real model yet\n",
        "# REMOVE this section once you have your actual model training/loading code\n",
        "# Defining the DummyModel class again or importing it if defined elsewhere\n",
        "class DummyModel:\n",
        "    def predict(self, X):\n",
        "        print(\"This is a dummy predict method.\")\n",
        "        return [0] * len(X)\n",
        "\n",
        "model = DummyModel()\n",
        "# END of Dummy Model Example - Please replace with your actual model\n",
        "\n",
        "\n",
        "# Save the model\n",
        "try:\n",
        "    joblib.dump(model, \"best_model.joblib\")\n",
        "    print(\"Model saved successfully to best_model.joblib\")\n",
        "except NameError:\n",
        "    # This specific NameError should be caught by the definition above,\n",
        "    # but keeping a general error catch is good practice.\n",
        "    print(\"Error: 'model' is not defined. Please ensure you have trained or loaded a model into a variable named 'model'.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while saving the model with joblib: {e}\")\n",
        "\n",
        "\n",
        "# Load the model later\n",
        "try:\n",
        "    loaded_model = joblib.load(\"best_model.joblib\")\n",
        "    print(\"Model loaded successfully from best_model.joblib\")\n",
        "    # You can now use loaded_model\n",
        "    # For example: loaded_model.predict(...)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'best_model.joblib' not found. Please ensure the save operation was successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the model with joblib: {e}\")"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- The heatmap provides a clear visualization of the relationships between different variables, making it easier to identify strong and weak correlations.\n",
        "- Features with high correlation can indicate redundancy in the dataset, which helps in refining predictive models by selecting only the most relevant variables.\n",
        "- Variables that show unexpected correlations may highlight underlying trends or issues, such as data inconsistencies or hidden dependencies.\n",
        "- Understanding correlations can assist in strategic decision-making, whether for business optimization, safety planning, or operational improvements.\n",
        "- The heatmap supports risk assessment by revealing connections between factors that may contribute to specific outcomes, helping to develop preventive measures.\n",
        "- Insights from the correlation matrix can guide feature engineering, allowing better data transformation for machine learning models.\n",
        "- If certain features show weak correlations, they may be less significant for prediction models, making it possible to simplify analyses without losing accuracy.\n",
        "- This visualization aids in anomaly detection, as extreme correlations might indicate biases or irregularities in the dataset that require further investigation.\n",
        "- Organizations can leverage these insights for smarter resource allocation, whether improving security measures, adjusting business strategies, or refining service delivery."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}